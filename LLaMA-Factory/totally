[2024-10-07 22:08:55,183] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10/07/2024 22:08:59 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:731] 2024-10-07 22:08:59,028 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 22:08:59,029 >> Model config LlamaConfig {
  "_name_or_path": "/home/sth/data/code/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,030 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,030 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,030 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,030 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2024-10-07 22:08:59,305 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:731] 2024-10-07 22:08:59,306 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 22:08:59,307 >> Model config LlamaConfig {
  "_name_or_path": "/home/sth/data/code/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,308 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,308 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,308 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 22:08:59,308 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2024-10-07 22:08:59,575 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
10/07/2024 22:08:59 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
10/07/2024 22:08:59 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
10/07/2024 22:08:59 - INFO - llamafactory.data.loader - Loading dataset culture_random_all_totally.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 62127 examples [00:00, 131197.85 examples/s]Generating train split: 62127 examples [00:00, 131111.17 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/62127 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 1514/62127 [00:00<00:04, 15089.11 examples/s]Converting format of dataset (num_proc=16):  39%|███▉      | 24104/62127 [00:00<00:00, 136419.61 examples/s]Converting format of dataset (num_proc=16):  77%|███████▋  | 47823/62127 [00:00<00:00, 181882.24 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 62127/62127 [00:00<00:00, 116016.49 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/62127 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   2%|▏         | 1000/62127 [00:00<00:58, 1048.15 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 2000/62127 [00:01<00:28, 2105.66 examples/s]Running tokenizer on dataset (num_proc=16):   5%|▍         | 3000/62127 [00:01<00:19, 3041.81 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 4000/62127 [00:01<00:14, 3918.85 examples/s]Running tokenizer on dataset (num_proc=16):  10%|▉         | 6000/62127 [00:01<00:08, 6334.22 examples/s]Running tokenizer on dataset (num_proc=16):  11%|█▏        | 7000/62127 [00:01<00:08, 6471.70 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 10000/62127 [00:01<00:05, 10128.09 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 12000/62127 [00:01<00:04, 11120.45 examples/s]Running tokenizer on dataset (num_proc=16):  24%|██▍       | 15000/62127 [00:02<00:03, 13713.38 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 19000/62127 [00:02<00:02, 17337.81 examples/s]Running tokenizer on dataset (num_proc=16):  40%|████      | 24883/62127 [00:02<00:01, 25947.41 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 28649/62127 [00:02<00:01, 27564.43 examples/s]Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 32532/62127 [00:02<00:01, 26131.67 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 35415/62127 [00:02<00:01, 25505.23 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▏   | 38415/62127 [00:02<00:00, 24543.10 examples/s]Running tokenizer on dataset (num_proc=16):  68%|██████▊   | 42298/62127 [00:03<00:00, 24870.53 examples/s]Running tokenizer on dataset (num_proc=16):  74%|███████▍  | 46064/62127 [00:03<00:00, 25156.20 examples/s]Running tokenizer on dataset (num_proc=16):  79%|███████▉  | 48947/62127 [00:03<00:00, 24032.84 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 51830/62127 [00:03<00:00, 22806.15 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 54713/62127 [00:03<00:00, 22069.54 examples/s]Running tokenizer on dataset (num_proc=16):  93%|█████████▎| 57596/62127 [00:03<00:00, 19354.21 examples/s]Running tokenizer on dataset (num_proc=16):  97%|█████████▋| 60362/62127 [00:04<00:00, 15212.48 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 62127/62127 [00:04<00:00, 11855.57 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 62127/62127 [00:04<00:00, 13822.53 examples/s]
[INFO|configuration_utils.py:731] 2024-10-07 22:09:07,539 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 22:09:07,540 >> Model config LlamaConfig {
  "_name_or_path": "/home/sth/data/code/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3675] 2024-10-07 22:09:07,566 >> loading weights file /home/sth/data/code/Meta-Llama-3-8B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1606] 2024-10-07 22:09:07,566 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2024-10-07 22:09:07,568 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

training example:
input_ids:
[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 1972, 1732, 449, 459, 3778, 13042, 4092, 13, 5321, 5266, 704, 279, 4435, 26028, 24507, 323, 4320, 279, 4860, 27136, 4184, 311, 701, 1866, 907, 1887, 13, 128009, 128006, 882, 128007, 271, 22818, 264, 674, 14924, 323, 674, 3883, 11, 5268, 279, 3072, 430, 1888, 5398, 82, 449, 701, 1866, 907, 1887, 311, 4320, 279, 3488, 627, 2, 14924, 25, 3277, 13176, 264, 3575, 477, 4360, 304, 701, 4029, 11, 902, 1912, 477, 7471, 1053, 499, 2543, 311, 369, 1520, 25, 264, 2254, 12818, 15360, 11, 264, 15481, 2536, 28926, 7471, 11, 264, 5426, 23693, 11, 477, 7000, 315, 279, 3485, 5380, 2, 3883, 25, 220, 16, 885, 2254, 12818, 15360, 220, 17, 885, 15481, 2536, 28926, 7471, 220, 18, 885, 5426, 23693, 220, 19, 18982, 315, 279, 3485, 198, 5618, 471, 279, 1396, 315, 279, 4183, 3072, 1193, 13, 128009, 128006, 78191, 128007, 271, 16, 128009]
inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a real person with an American cultural background. Please fill out the World Values Survey and answer the questions honestly according to your own value system.<|eot_id|><|start_header_id|>user<|end_header_id|>

Given a #Question and #Options, choose the option that best aligns with your own value system to answer the question.
#Question: When facing a problem or issue in your community, which group or organization would you turn to for help: a local neighborhood association, a regional non-profit organization, a national charity, or none of the above?
#Options: 1.A local neighborhood association 2.A regional non-profit organization 3.A national charity 4.None of the above
Please return the number of the selected option only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

1<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 128009]
labels:
1<|eot_id|>
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
[INFO|modeling_utils.py:4507] 2024-10-07 22:09:12,520 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4515] 2024-10-07 22:09:12,520 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/sth/data/code/Meta-Llama-3-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:991] 2024-10-07 22:09:12,523 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1038] 2024-10-07 22:09:12,523 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128009
  ],
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}

10/07/2024 22:09:12 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
10/07/2024 22:09:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
10/07/2024 22:09:12 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
10/07/2024 22:09:12 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
10/07/2024 22:09:12 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,v_proj,o_proj,gate_proj,k_proj,up_proj,q_proj
10/07/2024 22:09:13 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:648] 2024-10-07 22:09:13,181 >> Using auto half precision backend
[INFO|trainer.py:2134] 2024-10-07 22:09:13,475 >> ***** Running training *****
[INFO|trainer.py:2135] 2024-10-07 22:09:13,475 >>   Num examples = 62,127
[INFO|trainer.py:2136] 2024-10-07 22:09:13,475 >>   Num Epochs = 1
[INFO|trainer.py:2137] 2024-10-07 22:09:13,475 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2140] 2024-10-07 22:09:13,475 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2141] 2024-10-07 22:09:13,475 >>   Gradient Accumulation steps = 32
[INFO|trainer.py:2142] 2024-10-07 22:09:13,475 >>   Total optimization steps = 485
[INFO|trainer.py:2143] 2024-10-07 22:09:13,480 >>   Number of trainable parameters = 20,971,520
[INFO|integration_utils.py:807] 2024-10-07 22:09:13,487 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: beiweixiaoxu. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.0
wandb: Run data is saved locally in /data/syxu/culture_steering/LLaMA-Factory/wandb/run-20241007_220915-hyk15ws4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run saves/llama3_lora_sft_random_all_totally
wandb: ⭐️ View project at https://wandb.ai/beiweixiaoxu/huggingface
wandb: 🚀 View run at https://wandb.ai/beiweixiaoxu/huggingface/runs/hyk15ws4
  0%|          | 0/485 [00:00<?, ?it/s]  0%|          | 1/485 [00:11<1:29:19, 11.07s/it]  0%|          | 2/485 [00:21<1:24:13, 10.46s/it]  1%|          | 3/485 [00:31<1:22:45, 10.30s/it]  1%|          | 4/485 [00:41<1:22:13, 10.26s/it]  1%|          | 5/485 [00:51<1:21:38, 10.21s/it]  1%|          | 6/485 [01:01<1:21:55, 10.26s/it]  1%|▏         | 7/485 [01:12<1:22:11, 10.32s/it]  2%|▏         | 8/485 [01:22<1:22:17, 10.35s/it]  2%|▏         | 9/485 [01:33<1:22:02, 10.34s/it]  2%|▏         | 10/485 [01:43<1:22:00, 10.36s/it]                                                    2%|▏         | 10/485 [01:43<1:22:00, 10.36s/it]  2%|▏         | 11/485 [01:53<1:22:10, 10.40s/it]  2%|▏         | 12/485 [02:04<1:21:54, 10.39s/it]  3%|▎         | 13/485 [02:14<1:21:42, 10.39s/it]  3%|▎         | 14/485 [02:25<1:22:38, 10.53s/it]  3%|▎         | 15/485 [02:36<1:23:06, 10.61s/it]  3%|▎         | 16/485 [02:46<1:22:34, 10.56s/it]  4%|▎         | 17/485 [02:57<1:22:25, 10.57s/it]  4%|▎         | 18/485 [03:08<1:22:37, 10.62s/it]  4%|▍         | 19/485 [03:18<1:22:18, 10.60s/it]  4%|▍         | 20/485 [03:29<1:21:32, 10.52s/it]                                                    4%|▍         | 20/485 [03:29<1:21:32, 10.52s/it]  4%|▍         | 21/485 [03:39<1:21:13, 10.50s/it]  5%|▍         | 22/485 [03:50<1:21:28, 10.56s/it]  5%|▍         | 23/485 [04:00<1:21:25, 10.58s/it]  5%|▍         | 24/485 [04:11<1:21:41, 10.63s/it]  5%|▌         | 25/485 [04:22<1:21:07, 10.58s/it]  5%|▌         | 26/485 [04:32<1:20:16, 10.49s/it]  6%|▌         | 27/485 [04:43<1:20:48, 10.59s/it]  6%|▌         | 28/485 [04:53<1:20:21, 10.55s/it]  6%|▌         | 29/485 [05:04<1:20:18, 10.57s/it]  6%|▌         | 30/485 [05:14<1:20:01, 10.55s/it]                                                    6%|▌         | 30/485 [05:14<1:20:01, 10.55s/it]  6%|▋         | 31/485 [05:25<1:19:35, 10.52s/it]  7%|▋         | 32/485 [05:35<1:19:16, 10.50s/it]  7%|▋         | 33/485 [05:46<1:19:18, 10.53s/it]  7%|▋         | 34/485 [05:56<1:19:22, 10.56s/it]  7%|▋         | 35/485 [06:07<1:19:20, 10.58s/it]  7%|▋         | 36/485 [06:17<1:18:44, 10.52s/it]  8%|▊         | 37/485 [06:28<1:18:36, 10.53s/it]  8%|▊         | 38/485 [06:38<1:18:32, 10.54s/it]  8%|▊         | 39/485 [06:49<1:18:06, 10.51s/it]  8%|▊         | 40/485 [06:59<1:18:04, 10.53s/it]                                                    8%|▊         | 40/485 [06:59<1:18:04, 10.53s/it]  8%|▊         | 41/485 [07:10<1:18:25, 10.60s/it]  9%|▊         | 42/485 [07:21<1:17:39, 10.52s/it]  9%|▉         | 43/485 [07:31<1:17:09, 10.47s/it]  9%|▉         | 44/485 [07:41<1:17:09, 10.50s/it]  9%|▉         | 45/485 [07:52<1:16:49, 10.48s/it]  9%|▉         | 46/485 [08:02<1:16:17, 10.43s/it] 10%|▉         | 47/485 [08:13<1:16:20, 10.46s/it] 10%|▉         | 48/485 [08:23<1:16:26, 10.50s/it] 10%|█         | 49/485 [08:34<1:16:20, 10.51s/it] 10%|█         | 50/485 [08:44<1:16:22, 10.53s/it]                                                   10%|█         | 50/485 [08:44<1:16:22, 10.53s/it] 11%|█         | 51/485 [08:55<1:16:04, 10.52s/it] 11%|█         | 52/485 [09:05<1:15:43, 10.49s/it] 11%|█         | 53/485 [09:16<1:16:01, 10.56s/it] 11%|█         | 54/485 [09:27<1:15:53, 10.57s/it] 11%|█▏        | 55/485 [09:37<1:15:17, 10.51s/it] 12%|█▏        | 56/485 [09:47<1:15:00, 10.49s/it] 12%|█▏        | 57/485 [09:58<1:15:44, 10.62s/it] 12%|█▏        | 58/485 [10:09<1:15:53, 10.66s/it] 12%|█▏        | 59/485 [10:20<1:15:09, 10.58s/it] 12%|█▏        | 60/485 [10:30<1:15:05, 10.60s/it]                                                   12%|█▏        | 60/485 [10:30<1:15:05, 10.60s/it] 13%|█▎        | 61/485 [10:41<1:14:38, 10.56s/it] 13%|█▎        | 62/485 [10:51<1:14:00, 10.50s/it] 13%|█▎        | 63/485 [11:01<1:13:22, 10.43s/it] 13%|█▎        | 64/485 [11:12<1:13:51, 10.53s/it] 13%|█▎        | 65/485 [11:22<1:13:26, 10.49s/it] 14%|█▎        | 66/485 [11:33<1:13:46, 10.56s/it] 14%|█▍        | 67/485 [11:44<1:14:25, 10.68s/it] 14%|█▍        | 68/485 [11:55<1:13:41, 10.60s/it] 14%|█▍        | 69/485 [12:05<1:13:18, 10.57s/it] 14%|█▍        | 70/485 [12:15<1:12:31, 10.49s/it]                                                   14%|█▍        | 70/485 [12:15<1:12:31, 10.49s/it] 15%|█▍        | 71/485 [12:26<1:12:50, 10.56s/it] 15%|█▍        | 72/485 [12:37<1:12:44, 10.57s/it] 15%|█▌        | 73/485 [12:47<1:11:50, 10.46s/it] 15%|█▌        | 74/485 [12:57<1:11:33, 10.45s/it] 15%|█▌        | 75/485 [13:08<1:11:28, 10.46s/it] 16%|█▌        | 76/485 [13:18<1:11:35, 10.50s/it] 16%|█▌        | 77/485 [13:29<1:11:42, 10.55s/it] 16%|█▌        | 78/485 [13:40<1:11:22, 10.52s/it] 16%|█▋        | 79/485 [13:50<1:11:04, 10.50s/it] 16%|█▋        | 80/485 [14:00<1:10:35, 10.46s/it]                                                   16%|█▋        | 80/485 [14:00<1:10:35, 10.46s/it] 17%|█▋        | 81/485 [14:11<1:09:59, 10.40s/it] 17%|█▋        | 82/485 [14:21<1:09:18, 10.32s/it] 17%|█▋        | 83/485 [14:31<1:09:57, 10.44s/it] 17%|█▋        | 84/485 [14:42<1:09:31, 10.40s/it] 18%|█▊        | 85/485 [14:52<1:09:40, 10.45s/it] 18%|█▊        | 86/485 [15:03<1:09:25, 10.44s/it] 18%|█▊        | 87/485 [15:13<1:09:24, 10.46s/it] 18%|█▊        | 88/485 [15:24<1:09:04, 10.44s/it] 18%|█▊        | 89/485 [15:34<1:08:39, 10.40s/it] 19%|█▊        | 90/485 [15:45<1:09:00, 10.48s/it]                                                   19%|█▊        | 90/485 [15:45<1:09:00, 10.48s/it] 19%|█▉        | 91/485 [15:55<1:09:00, 10.51s/it] 19%|█▉        | 92/485 [16:06<1:08:49, 10.51s/it] 19%|█▉        | 93/485 [16:16<1:08:55, 10.55s/it] 19%|█▉        | 94/485 [16:27<1:08:21, 10.49s/it] 20%|█▉        | 95/485 [16:37<1:08:32, 10.55s/it] 20%|█▉        | 96/485 [16:48<1:08:00, 10.49s/it] 20%|██        | 97/485 [16:58<1:08:19, 10.57s/it] 20%|██        | 98/485 [17:09<1:07:59, 10.54s/it] 20%|██        | 99/485 [17:19<1:07:35, 10.51s/it] 21%|██        | 100/485 [17:30<1:06:52, 10.42s/it]                                                    21%|██        | 100/485 [17:30<1:06:52, 10.42s/it] 21%|██        | 101/485 [17:40<1:06:29, 10.39s/it] 21%|██        | 102/485 [17:50<1:06:35, 10.43s/it] 21%|██        | 103/485 [18:01<1:06:13, 10.40s/it] 21%|██▏       | 104/485 [18:11<1:05:52, 10.37s/it] 22%|██▏       | 105/485 [18:22<1:05:48, 10.39s/it] 22%|██▏       | 106/485 [18:32<1:05:57, 10.44s/it] 22%|██▏       | 107/485 [18:43<1:06:04, 10.49s/it] 22%|██▏       | 108/485 [18:53<1:06:27, 10.58s/it] 22%|██▏       | 109/485 [19:04<1:06:14, 10.57s/it] 23%|██▎       | 110/485 [19:14<1:05:43, 10.52s/it]                                                    23%|██▎       | 110/485 [19:14<1:05:43, 10.52s/it] 23%|██▎       | 111/485 [19:25<1:05:52, 10.57s/it] 23%|██▎       | 112/485 [19:35<1:04:58, 10.45s/it] 23%|██▎       | 113/485 [19:46<1:04:47, 10.45s/it] 24%|██▎       | 114/485 [19:56<1:05:09, 10.54s/it] 24%|██▎       | 115/485 [20:07<1:05:02, 10.55s/it] 24%|██▍       | 116/485 [20:17<1:04:28, 10.48s/it] 24%|██▍       | 117/485 [20:28<1:04:22, 10.50s/it] 24%|██▍       | 118/485 [20:38<1:03:45, 10.42s/it] 25%|██▍       | 119/485 [20:48<1:03:23, 10.39s/it] 25%|██▍       | 120/485 [20:59<1:03:22, 10.42s/it]                                                    25%|██▍       | 120/485 [20:59<1:03:22, 10.42s/it] 25%|██▍       | 121/485 [21:09<1:03:21, 10.44s/it] 25%|██▌       | 122/485 [21:20<1:03:01, 10.42s/it] 25%|██▌       | 123/485 [21:30<1:02:52, 10.42s/it] 26%|██▌       | 124/485 [21:41<1:02:40, 10.42s/it] 26%|██▌       | 125/485 [21:51<1:02:53, 10.48s/it] 26%|██▌       | 126/485 [22:02<1:02:57, 10.52s/it] 26%|██▌       | 127/485 [22:12<1:02:43, 10.51s/it] 26%|██▋       | 128/485 [22:23<1:02:04, 10.43s/it] 27%|██▋       | 129/485 [22:33<1:01:54, 10.43s/it] 27%|██▋       | 130/485 [22:44<1:01:45, 10.44s/it]                                                    27%|██▋       | 130/485 [22:44<1:01:45, 10.44s/it] 27%|██▋       | 131/485 [22:54<1:01:26, 10.41s/it] 27%|██▋       | 132/485 [23:04<1:01:01, 10.37s/it] 27%|██▋       | 133/485 [23:15<1:01:09, 10.42s/it] 28%|██▊       | 134/485 [23:25<1:01:17, 10.48s/it] 28%|██▊       | 135/485 [23:36<1:01:44, 10.58s/it] 28%|██▊       | 136/485 [23:47<1:02:07, 10.68s/it] 28%|██▊       | 137/485 [23:58<1:01:40, 10.63s/it] 28%|██▊       | 138/485 [24:08<1:01:19, 10.60s/it] 29%|██▊       | 139/485 [24:19<1:01:01, 10.58s/it] 29%|██▉       | 140/485 [24:29<1:00:58, 10.60s/it]                                                    29%|██▉       | 140/485 [24:29<1:00:58, 10.60s/it] 29%|██▉       | 141/485 [24:40<1:00:42, 10.59s/it] 29%|██▉       | 142/485 [24:50<1:00:32, 10.59s/it] 29%|██▉       | 143/485 [25:01<1:00:35, 10.63s/it] 30%|██▉       | 144/485 [25:12<1:00:20, 10.62s/it] 30%|██▉       | 145/485 [25:22<59:43, 10.54s/it]   30%|███       | 146/485 [25:33<59:37, 10.55s/it] 30%|███       | 147/485 [25:43<59:22, 10.54s/it] 31%|███       | 148/485 [25:54<59:06, 10.52s/it] 31%|███       | 149/485 [26:04<58:18, 10.41s/it] 31%|███       | 150/485 [26:14<58:18, 10.44s/it]                                                  31%|███       | 150/485 [26:14<58:18, 10.44s/it] 31%|███       | 151/485 [26:25<58:21, 10.48s/it] 31%|███▏      | 152/485 [26:36<58:29, 10.54s/it] 32%|███▏      | 153/485 [26:46<58:19, 10.54s/it] 32%|███▏      | 154/485 [26:57<58:02, 10.52s/it] 32%|███▏      | 155/485 [27:07<58:00, 10.55s/it] 32%|███▏      | 156/485 [27:18<57:54, 10.56s/it] 32%|███▏      | 157/485 [27:28<57:41, 10.55s/it] 33%|███▎      | 158/485 [27:39<57:37, 10.57s/it] 33%|███▎      | 159/485 [27:49<57:06, 10.51s/it] 33%|███▎      | 160/485 [28:00<57:04, 10.54s/it]                                                  33%|███▎      | 160/485 [28:00<57:04, 10.54s/it] 33%|███▎      | 161/485 [28:11<57:06, 10.58s/it] 33%|███▎      | 162/485 [28:21<56:44, 10.54s/it] 34%|███▎      | 163/485 [28:32<56:38, 10.56s/it] 34%|███▍      | 164/485 [28:42<56:18, 10.53s/it] 34%|███▍      | 165/485 [28:53<56:11, 10.53s/it] 34%|███▍      | 166/485 [29:03<56:07, 10.56s/it] 34%|███▍      | 167/485 [29:14<56:04, 10.58s/it] 35%|███▍      | 168/485 [29:24<55:29, 10.50s/it] 35%|███▍      | 169/485 [29:35<55:27, 10.53s/it] 35%|███▌      | 170/485 [29:45<54:51, 10.45s/it]                                                  35%|███▌      | 170/485 [29:45<54:51, 10.45s/it] 35%|███▌      | 171/485 [29:56<54:59, 10.51s/it] 35%|███▌      | 172/485 [30:06<54:50, 10.51s/it] 36%|███▌      | 173/485 [30:17<54:44, 10.53s/it] 36%|███▌      | 174/485 [30:27<54:17, 10.47s/it] 36%|███▌      | 175/485 [30:38<54:05, 10.47s/it] 36%|███▋      | 176/485 [30:48<54:08, 10.51s/it] 36%|███▋      | 177/485 [30:59<53:42, 10.46s/it] 37%|███▋      | 178/485 [31:09<53:13, 10.40s/it] 37%|███▋      | 179/485 [31:19<53:11, 10.43s/it] 37%|███▋      | 180/485 [31:30<53:18, 10.49s/it]                                                  37%|███▋      | 180/485 [31:30<53:18, 10.49s/it] 37%|███▋      | 181/485 [31:40<52:45, 10.41s/it] 38%|███▊      | 182/485 [31:51<52:51, 10.47s/it] 38%|███▊      | 183/485 [32:01<52:34, 10.45s/it] 38%|███▊      | 184/485 [32:12<52:43, 10.51s/it] 38%|███▊      | 185/485 [32:23<52:59, 10.60s/it] 38%|███▊      | 186/485 [32:33<52:30, 10.54s/it] 39%|███▊      | 187/485 [32:44<52:23, 10.55s/it] 39%|███▉      | 188/485 [32:54<51:58, 10.50s/it] 39%|███▉      | 189/485 [33:05<51:56, 10.53s/it] 39%|███▉      | 190/485 [33:15<51:40, 10.51s/it]                                                  39%|███▉      | 190/485 [33:15<51:40, 10.51s/it] 39%|███▉      | 191/485 [33:26<51:40, 10.54s/it] 40%|███▉      | 192/485 [33:36<51:23, 10.52s/it] 40%|███▉      | 193/485 [33:47<51:33, 10.59s/it] 40%|████      | 194/485 [33:57<51:15, 10.57s/it] 40%|████      | 195/485 [34:08<50:34, 10.46s/it] 40%|████      | 196/485 [34:18<50:32, 10.49s/it] 41%|████      | 197/485 [34:29<50:25, 10.51s/it] 41%|████      | 198/485 [34:39<49:48, 10.41s/it] 41%|████      | 199/485 [34:49<49:48, 10.45s/it] 41%|████      | 200/485 [35:00<49:59, 10.52s/it]                                                  41%|████      | 200/485 [35:00<49:59, 10.52s/it] 41%|████▏     | 201/485 [35:11<49:58, 10.56s/it] 42%|████▏     | 202/485 [35:21<49:34, 10.51s/it] 42%|████▏     | 203/485 [35:32<49:13, 10.47s/it] 42%|████▏     | 204/485 [35:42<49:11, 10.50s/it] 42%|████▏     | 205/485 [35:53<48:54, 10.48s/it] 42%|████▏     | 206/485 [36:03<48:38, 10.46s/it] 43%|████▎     | 207/485 [36:14<48:36, 10.49s/it] 43%|████▎     | 208/485 [36:24<48:30, 10.51s/it] 43%|████▎     | 209/485 [36:35<48:17, 10.50s/it] 43%|████▎     | 210/485 [36:45<48:27, 10.57s/it]                                                  43%|████▎     | 210/485 [36:45<48:27, 10.57s/it] 44%|████▎     | 211/485 [36:56<48:15, 10.57s/it] 44%|████▎     | 212/485 [37:06<47:57, 10.54s/it] 44%|████▍     | 213/485 [37:17<47:46, 10.54s/it] 44%|████▍     | 214/485 [37:28<47:44, 10.57s/it] 44%|████▍     | 215/485 [37:38<47:14, 10.50s/it] 45%|████▍     | 216/485 [37:48<46:58, 10.48s/it] 45%|████▍     | 217/485 [37:59<46:47, 10.48s/it] 45%|████▍     | 218/485 [38:09<46:49, 10.52s/it] 45%|████▌     | 219/485 [38:20<46:38, 10.52s/it] 45%|████▌     | 220/485 [38:30<46:12, 10.46s/it]                                                  45%|████▌     | 220/485 [38:30<46:12, 10.46s/it] 46%|████▌     | 221/485 [38:41<46:09, 10.49s/it] 46%|████▌     | 222/485 [38:51<45:59, 10.49s/it] 46%|████▌     | 223/485 [39:02<45:30, 10.42s/it] 46%|████▌     | 224/485 [39:12<45:30, 10.46s/it] 46%|████▋     | 225/485 [39:23<45:14, 10.44s/it] 47%|████▋     | 226/485 [39:33<45:11, 10.47s/it] 47%|████▋     | 227/485 [39:44<45:09, 10.50s/it] 47%|████▋     | 228/485 [39:54<44:49, 10.47s/it] 47%|████▋     | 229/485 [40:05<45:05, 10.57s/it] 47%|████▋     | 230/485 [40:15<44:56, 10.58s/it]                                                  47%|████▋     | 230/485 [40:15<44:56, 10.58s/it] 48%|████▊     | 231/485 [40:26<44:50, 10.59s/it] 48%|████▊     | 232/485 [40:37<44:32, 10.56s/it] 48%|████▊     | 233/485 [40:47<44:09, 10.51s/it] 48%|████▊     | 234/485 [40:57<43:46, 10.46s/it] 48%|████▊     | 235/485 [41:08<43:37, 10.47s/it] 49%|████▊     | 236/485 [41:18<43:22, 10.45s/it] 49%|████▉     | 237/485 [41:29<43:13, 10.46s/it] 49%|████▉     | 238/485 [41:39<43:02, 10.45s/it] 49%|████▉     | 239/485 [41:50<42:56, 10.47s/it] 49%|████▉     | 240/485 [42:00<42:57, 10.52s/it]                                                  49%|████▉     | 240/485 [42:00<42:57, 10.52s/it] 50%|████▉     | 241/485 [42:11<43:00, 10.58s/it] 50%|████▉     | 242/485 [42:21<42:33, 10.51s/it] 50%|█████     | 243/485 [42:32<42:44, 10.60s/it] 50%|█████     | 244/485 [42:43<42:27, 10.57s/it] 51%|█████     | 245/485 [42:53<42:11, 10.55s/it] 51%|█████     | 246/485 [43:04<41:55, 10.52s/it] 51%|█████     | 247/485 [43:15<42:18, 10.67s/it] 51%|█████     | 248/485 [43:25<41:50, 10.59s/it] 51%|█████▏    | 249/485 [43:35<41:29, 10.55s/it] 52%|█████▏    | 250/485 [43:46<41:02, 10.48s/it]                                                  52%|█████▏    | 250/485 [43:46<41:02, 10.48s/it] 52%|█████▏    | 251/485 [43:56<40:59, 10.51s/it] 52%|█████▏    | 252/485 [44:07<40:48, 10.51s/it] 52%|█████▏    | 253/485 [44:18<40:50, 10.56s/it] 52%|█████▏    | 254/485 [44:28<40:31, 10.53s/it] 53%|█████▎    | 255/485 [44:39<40:28, 10.56s/it] 53%|█████▎    | 256/485 [44:49<40:22, 10.58s/it] 53%|█████▎    | 257/485 [45:00<40:05, 10.55s/it] 53%|█████▎    | 258/485 [45:10<39:55, 10.55s/it] 53%|█████▎    | 259/485 [45:21<39:44, 10.55s/it] 54%|█████▎    | 260/485 [45:31<39:27, 10.52s/it]                                                  54%|█████▎    | 260/485 [45:31<39:27, 10.52s/it] 54%|█████▍    | 261/485 [45:42<39:20, 10.54s/it] 54%|█████▍    | 262/485 [45:52<39:13, 10.55s/it] 54%|█████▍    | 263/485 [46:03<38:52, 10.51s/it] 54%|█████▍    | 264/485 [46:13<38:44, 10.52s/it] 55%|█████▍    | 265/485 [46:24<38:44, 10.57s/it] 55%|█████▍    | 266/485 [46:35<38:30, 10.55s/it] 55%|█████▌    | 267/485 [46:45<38:10, 10.51s/it] 55%|█████▌    | 268/485 [46:55<37:52, 10.47s/it] 55%|█████▌    | 269/485 [47:06<37:32, 10.43s/it] 56%|█████▌    | 270/485 [47:16<37:23, 10.44s/it]                                                  56%|█████▌    | 270/485 [47:16<37:23, 10.44s/it] 56%|█████▌    | 271/485 [47:27<37:22, 10.48s/it] 56%|█████▌    | 272/485 [47:37<37:03, 10.44s/it] 56%|█████▋    | 273/485 [47:48<36:54, 10.45s/it] 56%|█████▋    | 274/485 [47:58<36:39, 10.42s/it] 57%|█████▋    | 275/485 [48:09<36:40, 10.48s/it] 57%|█████▋    | 276/485 [48:19<36:24, 10.45s/it] 57%|█████▋    | 277/485 [48:30<36:27, 10.52s/it] 57%|█████▋    | 278/485 [48:40<36:05, 10.46s/it] 58%|█████▊    | 279/485 [48:50<35:48, 10.43s/it] 58%|█████▊    | 280/485 [49:01<36:03, 10.55s/it]                                                  58%|█████▊    | 280/485 [49:01<36:03, 10.55s/it] 58%|█████▊    | 281/485 [49:11<35:42, 10.50s/it] 58%|█████▊    | 282/485 [49:22<35:45, 10.57s/it] 58%|█████▊    | 283/485 [49:33<35:29, 10.54s/it] 59%|█████▊    | 284/485 [49:43<35:07, 10.49s/it] 59%|█████▉    | 285/485 [49:53<34:53, 10.47s/it] 59%|█████▉    | 286/485 [50:04<35:13, 10.62s/it] 59%|█████▉    | 287/485 [50:15<34:46, 10.54s/it] 59%|█████▉    | 288/485 [50:25<34:44, 10.58s/it] 60%|█████▉    | 289/485 [50:36<34:25, 10.54s/it] 60%|█████▉    | 290/485 [50:46<34:14, 10.54s/it]                                                  60%|█████▉    | 290/485 [50:46<34:14, 10.54s/it] 60%|██████    | 291/485 [50:57<34:10, 10.57s/it] 60%|██████    | 292/485 [51:08<33:53, 10.54s/it] 60%|██████    | 293/485 [51:18<33:33, 10.49s/it] 61%|██████    | 294/485 [51:29<33:37, 10.56s/it] 61%|██████    | 295/485 [51:39<33:17, 10.52s/it] 61%|██████    | 296/485 [51:50<33:19, 10.58s/it] 61%|██████    | 297/485 [52:00<33:00, 10.53s/it] 61%|██████▏   | 298/485 [52:11<32:51, 10.54s/it] 62%|██████▏   | 299/485 [52:21<32:44, 10.56s/it] 62%|██████▏   | 300/485 [52:32<32:43, 10.61s/it]                                                  62%|██████▏   | 300/485 [52:32<32:43, 10.61s/it] 62%|██████▏   | 301/485 [52:43<32:33, 10.62s/it] 62%|██████▏   | 302/485 [52:54<32:31, 10.66s/it] 62%|██████▏   | 303/485 [53:04<32:21, 10.67s/it] 63%|██████▎   | 304/485 [53:15<32:11, 10.67s/it] 63%|██████▎   | 305/485 [53:26<32:06, 10.70s/it] 63%|██████▎   | 306/485 [53:36<31:41, 10.62s/it] 63%|██████▎   | 307/485 [53:47<31:37, 10.66s/it] 64%|██████▎   | 308/485 [53:58<31:39, 10.73s/it] 64%|██████▎   | 309/485 [54:08<31:19, 10.68s/it] 64%|██████▍   | 310/485 [54:19<31:04, 10.65s/it]                                                  64%|██████▍   | 310/485 [54:19<31:04, 10.65s/it] 64%|██████▍   | 311/485 [54:29<30:50, 10.64s/it] 64%|██████▍   | 312/485 [54:40<30:41, 10.65s/it] 65%|██████▍   | 313/485 [54:51<30:38, 10.69s/it] 65%|██████▍   | 314/485 [55:01<30:19, 10.64s/it] 65%|██████▍   | 315/485 [55:12<29:59, 10.59s/it] 65%|██████▌   | 316/485 [55:22<29:44, 10.56s/it] 65%|██████▌   | 317/485 [55:33<29:34, 10.56s/it] 66%|██████▌   | 318/485 [55:43<29:15, 10.51s/it] 66%|██████▌   | 319/485 [55:54<28:57, 10.47s/it] 66%|██████▌   | 320/485 [56:04<28:53, 10.51s/it]                                                  66%|██████▌   | 320/485 [56:04<28:53, 10.51s/it] 66%|██████▌   | 321/485 [56:15<28:43, 10.51s/it] 66%|██████▋   | 322/485 [56:26<28:55, 10.65s/it] 67%|██████▋   | 323/485 [56:36<28:44, 10.64s/it] 67%|██████▋   | 324/485 [56:47<28:18, 10.55s/it] 67%|██████▋   | 325/485 [56:57<28:08, 10.55s/it] 67%|██████▋   | 326/485 [57:08<27:51, 10.51s/it] 67%|██████▋   | 327/485 [57:19<27:51, 10.58s/it] 68%|██████▊   | 328/485 [57:29<27:35, 10.55s/it] 68%|██████▊   | 329/485 [57:40<27:30, 10.58s/it] 68%|██████▊   | 330/485 [57:50<27:25, 10.62s/it]                                                  68%|██████▊   | 330/485 [57:50<27:25, 10.62s/it] 68%|██████▊   | 331/485 [58:01<27:14, 10.61s/it] 68%|██████▊   | 332/485 [58:11<27:00, 10.59s/it] 69%|██████▊   | 333/485 [58:22<26:48, 10.58s/it] 69%|██████▉   | 334/485 [58:32<26:26, 10.51s/it] 69%|██████▉   | 335/485 [58:43<26:05, 10.44s/it] 69%|██████▉   | 336/485 [58:53<26:00, 10.47s/it] 69%|██████▉   | 337/485 [59:04<26:01, 10.55s/it] 70%|██████▉   | 338/485 [59:14<25:51, 10.55s/it] 70%|██████▉   | 339/485 [59:25<25:45, 10.59s/it] 70%|███████   | 340/485 [59:36<25:35, 10.59s/it]                                                  70%|███████   | 340/485 [59:36<25:35, 10.59s/it] 70%|███████   | 341/485 [59:47<25:32, 10.64s/it] 71%|███████   | 342/485 [59:57<25:08, 10.55s/it] 71%|███████   | 343/485 [1:00:07<24:59, 10.56s/it] 71%|███████   | 344/485 [1:00:18<24:51, 10.58s/it] 71%|███████   | 345/485 [1:00:28<24:34, 10.53s/it] 71%|███████▏  | 346/485 [1:00:39<24:19, 10.50s/it] 72%|███████▏  | 347/485 [1:00:50<24:21, 10.59s/it] 72%|███████▏  | 348/485 [1:01:00<24:04, 10.55s/it] 72%|███████▏  | 349/485 [1:01:10<23:45, 10.48s/it] 72%|███████▏  | 350/485 [1:01:21<23:43, 10.55s/it]                                                    72%|███████▏  | 350/485 [1:01:21<23:43, 10.55s/it] 72%|███████▏  | 351/485 [1:01:32<23:34, 10.56s/it] 73%|███████▎  | 352/485 [1:01:42<23:14, 10.49s/it] 73%|███████▎  | 353/485 [1:01:52<23:00, 10.46s/it] 73%|███████▎  | 354/485 [1:02:03<22:52, 10.48s/it] 73%|███████▎  | 355/485 [1:02:13<22:39, 10.46s/it] 73%|███████▎  | 356/485 [1:02:24<22:26, 10.44s/it] 74%|███████▎  | 357/485 [1:02:34<22:07, 10.37s/it] 74%|███████▍  | 358/485 [1:02:45<22:03, 10.42s/it] 74%|███████▍  | 359/485 [1:02:55<21:49, 10.39s/it] 74%|███████▍  | 360/485 [1:03:05<21:39, 10.39s/it]                                                    74%|███████▍  | 360/485 [1:03:05<21:39, 10.39s/it] 74%|███████▍  | 361/485 [1:03:16<21:31, 10.42s/it] 75%|███████▍  | 362/485 [1:03:26<21:26, 10.46s/it] 75%|███████▍  | 363/485 [1:03:37<21:30, 10.58s/it] 75%|███████▌  | 364/485 [1:03:48<21:32, 10.68s/it] 75%|███████▌  | 365/485 [1:03:58<21:10, 10.59s/it] 75%|███████▌  | 366/485 [1:04:09<20:47, 10.48s/it] 76%|███████▌  | 367/485 [1:04:19<20:39, 10.51s/it] 76%|███████▌  | 368/485 [1:04:30<20:31, 10.52s/it] 76%|███████▌  | 369/485 [1:04:40<20:18, 10.51s/it] 76%|███████▋  | 370/485 [1:04:51<20:05, 10.49s/it]                                                    76%|███████▋  | 370/485 [1:04:51<20:05, 10.49s/it] 76%|███████▋  | 371/485 [1:05:01<20:04, 10.57s/it] 77%|███████▋  | 372/485 [1:05:12<19:48, 10.51s/it] 77%|███████▋  | 373/485 [1:05:22<19:36, 10.51s/it] 77%|███████▋  | 374/485 [1:05:33<19:29, 10.53s/it] 77%|███████▋  | 375/485 [1:05:43<19:16, 10.51s/it] 78%|███████▊  | 376/485 [1:05:54<19:00, 10.47s/it] 78%|███████▊  | 377/485 [1:06:04<18:54, 10.50s/it] 78%|███████▊  | 378/485 [1:06:15<18:42, 10.49s/it] 78%|███████▊  | 379/485 [1:06:25<18:36, 10.53s/it] 78%|███████▊  | 380/485 [1:06:36<18:18, 10.46s/it]                                                    78%|███████▊  | 380/485 [1:06:36<18:18, 10.46s/it] 79%|███████▊  | 381/485 [1:06:46<18:11, 10.49s/it] 79%|███████▉  | 382/485 [1:06:57<18:00, 10.49s/it] 79%|███████▉  | 383/485 [1:07:07<17:42, 10.42s/it] 79%|███████▉  | 384/485 [1:07:18<17:33, 10.43s/it] 79%|███████▉  | 385/485 [1:07:28<17:19, 10.39s/it] 80%|███████▉  | 386/485 [1:07:38<17:12, 10.43s/it] 80%|███████▉  | 387/485 [1:07:49<17:05, 10.47s/it] 80%|████████  | 388/485 [1:08:00<16:59, 10.51s/it] 80%|████████  | 389/485 [1:08:10<16:50, 10.53s/it] 80%|████████  | 390/485 [1:08:21<16:40, 10.54s/it]                                                    80%|████████  | 390/485 [1:08:21<16:40, 10.54s/it] 81%|████████  | 391/485 [1:08:31<16:33, 10.57s/it] 81%|████████  | 392/485 [1:08:42<16:16, 10.50s/it] 81%|████████  | 393/485 [1:08:52<16:03, 10.48s/it] 81%|████████  | 394/485 [1:09:02<15:51, 10.45s/it] 81%|████████▏ | 395/485 [1:09:13<15:43, 10.48s/it] 82%|████████▏ | 396/485 [1:09:24<15:40, 10.57s/it] 82%|████████▏ | 397/485 [1:09:34<15:28, 10.55s/it] 82%|████████▏ | 398/485 [1:09:45<15:09, 10.46s/it] 82%|████████▏ | 399/485 [1:09:55<14:56, 10.42s/it] 82%|████████▏ | 400/485 [1:10:05<14:43, 10.40s/it]                                                    82%|████████▏ | 400/485 [1:10:05<14:43, 10.40s/it] 83%|████████▎ | 401/485 [1:10:16<14:32, 10.39s/it] 83%|████████▎ | 402/485 [1:10:26<14:20, 10.36s/it] 83%|████████▎ | 403/485 [1:10:36<14:12, 10.40s/it] 83%|████████▎ | 404/485 [1:10:47<14:04, 10.42s/it] 84%|████████▎ | 405/485 [1:10:57<13:52, 10.40s/it] 84%|████████▎ | 406/485 [1:11:08<13:43, 10.42s/it] 84%|████████▍ | 407/485 [1:11:18<13:32, 10.41s/it] 84%|████████▍ | 408/485 [1:11:29<13:26, 10.47s/it] 84%|████████▍ | 409/485 [1:11:39<13:12, 10.43s/it] 85%|████████▍ | 410/485 [1:11:50<13:05, 10.48s/it]                                                    85%|████████▍ | 410/485 [1:11:50<13:05, 10.48s/it] 85%|████████▍ | 411/485 [1:12:00<12:55, 10.48s/it] 85%|████████▍ | 412/485 [1:12:10<12:39, 10.40s/it] 85%|████████▌ | 413/485 [1:12:21<12:25, 10.36s/it] 85%|████████▌ | 414/485 [1:12:31<12:12, 10.32s/it] 86%|████████▌ | 415/485 [1:12:41<12:08, 10.40s/it] 86%|████████▌ | 416/485 [1:12:52<12:00, 10.44s/it] 86%|████████▌ | 417/485 [1:13:02<11:49, 10.43s/it] 86%|████████▌ | 418/485 [1:13:13<11:39, 10.44s/it] 86%|████████▋ | 419/485 [1:13:23<11:27, 10.41s/it] 87%|████████▋ | 420/485 [1:13:33<11:14, 10.38s/it]                                                    87%|████████▋ | 420/485 [1:13:33<11:14, 10.38s/it] 87%|████████▋ | 421/485 [1:13:44<11:06, 10.41s/it] 87%|████████▋ | 422/485 [1:13:54<10:55, 10.41s/it] 87%|████████▋ | 423/485 [1:14:05<10:43, 10.38s/it] 87%|████████▋ | 424/485 [1:14:15<10:34, 10.40s/it] 88%|████████▊ | 425/485 [1:14:25<10:24, 10.40s/it] 88%|████████▊ | 426/485 [1:14:36<10:13, 10.40s/it] 88%|████████▊ | 427/485 [1:14:46<10:04, 10.43s/it] 88%|████████▊ | 428/485 [1:14:57<09:57, 10.48s/it] 88%|████████▊ | 429/485 [1:15:08<09:55, 10.63s/it] 89%|████████▊ | 430/485 [1:15:18<09:38, 10.52s/it]                                                    89%|████████▊ | 430/485 [1:15:18<09:38, 10.52s/it] 89%|████████▉ | 431/485 [1:15:29<09:29, 10.55s/it] 89%|████████▉ | 432/485 [1:15:39<09:16, 10.50s/it] 89%|████████▉ | 433/485 [1:15:50<09:05, 10.49s/it] 89%|████████▉ | 434/485 [1:16:00<09:00, 10.59s/it] 90%|████████▉ | 435/485 [1:16:11<08:49, 10.59s/it] 90%|████████▉ | 436/485 [1:16:22<08:37, 10.56s/it] 90%|█████████ | 437/485 [1:16:32<08:28, 10.59s/it] 90%|█████████ | 438/485 [1:16:43<08:18, 10.60s/it] 91%|█████████ | 439/485 [1:16:53<08:04, 10.54s/it] 91%|█████████ | 440/485 [1:17:04<07:52, 10.51s/it]                                                    91%|█████████ | 440/485 [1:17:04<07:52, 10.51s/it] 91%|█████████ | 441/485 [1:17:14<07:43, 10.54s/it] 91%|█████████ | 442/485 [1:17:25<07:34, 10.57s/it] 91%|█████████▏| 443/485 [1:17:35<07:22, 10.55s/it] 92%|█████████▏| 444/485 [1:17:46<07:13, 10.56s/it] 92%|█████████▏| 445/485 [1:17:57<07:04, 10.62s/it] 92%|█████████▏| 446/485 [1:18:07<06:53, 10.60s/it] 92%|█████████▏| 447/485 [1:18:18<06:43, 10.61s/it] 92%|█████████▏| 448/485 [1:18:28<06:30, 10.55s/it] 93%|█████████▎| 449/485 [1:18:39<06:19, 10.54s/it] 93%|█████████▎| 450/485 [1:18:49<06:07, 10.49s/it]                                                    93%|█████████▎| 450/485 [1:18:49<06:07, 10.49s/it] 93%|█████████▎| 451/485 [1:19:00<05:55, 10.45s/it] 93%|█████████▎| 452/485 [1:19:10<05:46, 10.49s/it] 93%|█████████▎| 453/485 [1:19:21<05:34, 10.44s/it] 94%|█████████▎| 454/485 [1:19:31<05:26, 10.53s/it] 94%|█████████▍| 455/485 [1:19:42<05:16, 10.56s/it] 94%|█████████▍| 456/485 [1:19:53<05:08, 10.62s/it] 94%|█████████▍| 457/485 [1:20:03<04:56, 10.58s/it] 94%|█████████▍| 458/485 [1:20:13<04:43, 10.51s/it] 95%|█████████▍| 459/485 [1:20:24<04:34, 10.54s/it] 95%|█████████▍| 460/485 [1:20:34<04:22, 10.49s/it]                                                    95%|█████████▍| 460/485 [1:20:34<04:22, 10.49s/it] 95%|█████████▌| 461/485 [1:20:45<04:12, 10.50s/it] 95%|█████████▌| 462/485 [1:20:56<04:01, 10.50s/it] 95%|█████████▌| 463/485 [1:21:06<03:50, 10.50s/it] 96%|█████████▌| 464/485 [1:21:17<03:40, 10.52s/it] 96%|█████████▌| 465/485 [1:21:27<03:29, 10.47s/it] 96%|█████████▌| 466/485 [1:21:37<03:19, 10.47s/it] 96%|█████████▋| 467/485 [1:21:48<03:07, 10.44s/it] 96%|█████████▋| 468/485 [1:21:58<02:57, 10.46s/it] 97%|█████████▋| 469/485 [1:22:09<02:46, 10.44s/it] 97%|█████████▋| 470/485 [1:22:19<02:36, 10.43s/it]                                                    97%|█████████▋| 470/485 [1:22:19<02:36, 10.43s/it] 97%|█████████▋| 471/485 [1:22:30<02:26, 10.45s/it] 97%|█████████▋| 472/485 [1:22:40<02:15, 10.45s/it] 98%|█████████▊| 473/485 [1:22:51<02:06, 10.53s/it] 98%|█████████▊| 474/485 [1:23:02<01:56, 10.62s/it] 98%|█████████▊| 475/485 [1:23:12<01:45, 10.56s/it] 98%|█████████▊| 476/485 [1:23:22<01:34, 10.52s/it] 98%|█████████▊| 477/485 [1:23:33<01:24, 10.55s/it] 99%|█████████▊| 478/485 [1:23:43<01:13, 10.52s/it] 99%|█████████▉| 479/485 [1:23:54<01:03, 10.56s/it] 99%|█████████▉| 480/485 [1:24:05<00:53, 10.61s/it]                                                    99%|█████████▉| 480/485 [1:24:05<00:53, 10.61s/it] 99%|█████████▉| 481/485 [1:24:15<00:42, 10.55s/it] 99%|█████████▉| 482/485 [1:24:26<00:31, 10.49s/it]100%|█████████▉| 483/485 [1:24:36<00:20, 10.46s/it]100%|█████████▉| 484/485 [1:24:47<00:10, 10.52s/it]100%|██████████| 485/485 [1:24:57<00:00, 10.50s/it][INFO|trainer.py:3503] 2024-10-07 23:34:14,936 >> Saving model checkpoint to saves/llama3_lora_sft_random_all_totally/checkpoint-485
[INFO|configuration_utils.py:731] 2024-10-07 23:34:14,958 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 23:34:14,959 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2024-10-07 23:34:15,094 >> tokenizer config file saved in saves/llama3_lora_sft_random_all_totally/checkpoint-485/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-10-07 23:34:15,094 >> Special tokens file saved in saves/llama3_lora_sft_random_all_totally/checkpoint-485/special_tokens_map.json
[INFO|trainer.py:2394] 2024-10-07 23:34:15,447 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 485/485 [1:24:58<00:00, 10.50s/it]100%|██████████| 485/485 [1:24:58<00:00, 10.51s/it]
[INFO|trainer.py:3503] 2024-10-07 23:34:15,451 >> Saving model checkpoint to saves/llama3_lora_sft_random_all_totally
[INFO|configuration_utils.py:731] 2024-10-07 23:34:15,473 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 23:34:15,474 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2024-10-07 23:34:15,594 >> tokenizer config file saved in saves/llama3_lora_sft_random_all_totally/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-10-07 23:34:15,594 >> Special tokens file saved in saves/llama3_lora_sft_random_all_totally/special_tokens_map.json
{'loss': 0.1774, 'grad_norm': 1.2788217067718506, 'learning_rate': 2.0408163265306123e-05, 'epoch': 0.02}
{'loss': 0.1279, 'grad_norm': 4.446345806121826, 'learning_rate': 4.0816326530612245e-05, 'epoch': 0.04}
{'loss': 0.1084, 'grad_norm': 1.526774287223816, 'learning_rate': 6.122448979591838e-05, 'epoch': 0.06}
{'loss': 0.0901, 'grad_norm': 2.574988603591919, 'learning_rate': 8.163265306122449e-05, 'epoch': 0.08}
{'loss': 0.107, 'grad_norm': 0.8911025524139404, 'learning_rate': 9.999870202927739e-05, 'epoch': 0.1}
{'loss': 0.1123, 'grad_norm': 1.9551070928573608, 'learning_rate': 9.984302706688962e-05, 'epoch': 0.12}
{'loss': 0.1024, 'grad_norm': 1.1288354396820068, 'learning_rate': 9.942868376961542e-05, 'epoch': 0.14}
{'loss': 0.1086, 'grad_norm': 1.1392101049423218, 'learning_rate': 9.875782243805019e-05, 'epoch': 0.16}
{'loss': 0.1007, 'grad_norm': 1.630719780921936, 'learning_rate': 9.783392461402208e-05, 'epoch': 0.19}
{'loss': 0.0982, 'grad_norm': 0.9048811197280884, 'learning_rate': 9.666178501257572e-05, 'epoch': 0.21}
{'loss': 0.1113, 'grad_norm': 1.5734295845031738, 'learning_rate': 9.524748663903406e-05, 'epoch': 0.23}
{'loss': 0.1061, 'grad_norm': 0.6758424043655396, 'learning_rate': 9.359836922027255e-05, 'epoch': 0.25}
{'loss': 0.0956, 'grad_norm': 0.9273124933242798, 'learning_rate': 9.172299111403641e-05, 'epoch': 0.27}
{'loss': 0.0889, 'grad_norm': 1.1649508476257324, 'learning_rate': 8.963108489397875e-05, 'epoch': 0.29}
{'loss': 0.0924, 'grad_norm': 1.0925921201705933, 'learning_rate': 8.733350684091806e-05, 'epoch': 0.31}
{'loss': 0.093, 'grad_norm': 1.0661073923110962, 'learning_rate': 8.484218060243815e-05, 'epoch': 0.33}
{'loss': 0.0802, 'grad_norm': 0.5675034523010254, 'learning_rate': 8.21700353132182e-05, 'epoch': 0.35}
{'loss': 0.0858, 'grad_norm': 1.6216312646865845, 'learning_rate': 7.933093849722724e-05, 'epoch': 0.37}
{'loss': 0.0868, 'grad_norm': 0.8116611242294312, 'learning_rate': 7.633962409999764e-05, 'epoch': 0.39}
{'loss': 0.0766, 'grad_norm': 1.4525057077407837, 'learning_rate': 7.321161602446602e-05, 'epoch': 0.41}
{'loss': 0.0874, 'grad_norm': 1.524718999862671, 'learning_rate': 6.99631475672041e-05, 'epoch': 0.43}
{'loss': 0.081, 'grad_norm': 1.0209393501281738, 'learning_rate': 6.661107717313823e-05, 'epoch': 0.45}
{'loss': 0.081, 'grad_norm': 0.897678792476654, 'learning_rate': 6.317280094596198e-05, 'epoch': 0.47}
{'loss': 0.0869, 'grad_norm': 0.8937944173812866, 'learning_rate': 5.966616236828263e-05, 'epoch': 0.49}
{'loss': 0.0825, 'grad_norm': 0.9075427055358887, 'learning_rate': 5.6109359700023653e-05, 'epoch': 0.52}
{'loss': 0.0629, 'grad_norm': 1.200230598449707, 'learning_rate': 5.252085153565375e-05, 'epoch': 0.54}
{'loss': 0.0715, 'grad_norm': 0.9143937230110168, 'learning_rate': 4.891926101036807e-05, 'epoch': 0.56}
{'loss': 0.0786, 'grad_norm': 0.9778908491134644, 'learning_rate': 4.5323279152359356e-05, 'epoch': 0.58}
{'loss': 0.0684, 'grad_norm': 0.5625537037849426, 'learning_rate': 4.175156788274738e-05, 'epoch': 0.6}
{'loss': 0.0615, 'grad_norm': 0.7916910648345947, 'learning_rate': 3.822266316656421e-05, 'epoch': 0.62}
{'loss': 0.0643, 'grad_norm': 0.8335567116737366, 'learning_rate': 3.4754878817408784e-05, 'epoch': 0.64}
{'loss': 0.0603, 'grad_norm': 1.1091606616973877, 'learning_rate': 3.1366211454991555e-05, 'epoch': 0.66}
{'loss': 0.076, 'grad_norm': 0.7162604331970215, 'learning_rate': 2.8074247108807567e-05, 'epoch': 0.68}
{'loss': 0.0535, 'grad_norm': 0.6017324328422546, 'learning_rate': 2.4896069952632788e-05, 'epoch': 0.7}
{'loss': 0.0647, 'grad_norm': 0.7452172636985779, 'learning_rate': 2.1848173643480872e-05, 'epoch': 0.72}
{'loss': 0.061, 'grad_norm': 0.8980852961540222, 'learning_rate': 1.894637572514058e-05, 'epoch': 0.74}
{'loss': 0.0623, 'grad_norm': 0.5712507367134094, 'learning_rate': 1.6205735540510676e-05, 'epoch': 0.76}
{'loss': 0.0521, 'grad_norm': 0.4058074653148651, 'learning_rate': 1.3640476078739295e-05, 'epoch': 0.78}
{'loss': 0.0536, 'grad_norm': 0.9348737001419067, 'learning_rate': 1.1263910162754221e-05, 'epoch': 0.8}
{'loss': 0.053, 'grad_norm': 1.0632966756820679, 'learning_rate': 9.088371360246106e-06, 'epoch': 0.82}
{'loss': 0.0581, 'grad_norm': 0.7279472947120667, 'learning_rate': 7.125149976652684e-06, 'epoch': 0.84}
{'loss': 0.0613, 'grad_norm': 0.8377360105514526, 'learning_rate': 5.384434462318777e-06, 'epoch': 0.87}
{'loss': 0.0649, 'grad_norm': 0.8943026065826416, 'learning_rate': 3.875258537909032e-06, 'epoch': 0.89}
{'loss': 0.062, 'grad_norm': 0.5262820720672607, 'learning_rate': 2.605454312474448e-06, 'epoch': 0.91}
{'loss': 0.0593, 'grad_norm': 1.6323881149291992, 'learning_rate': 1.5816116374737455e-06, 'epoch': 0.93}
{'loss': 0.0557, 'grad_norm': 0.9182388186454773, 'learning_rate': 8.090439076887557e-07, 'epoch': 0.95}
{'loss': 0.0602, 'grad_norm': 1.0050994157791138, 'learning_rate': 2.9176048651513577e-07, 'epoch': 0.97}
{'loss': 0.0542, 'grad_norm': 0.9982089400291443, 'learning_rate': 3.244589873185322e-08, 'epoch': 0.99}
{'train_runtime': 5101.968, 'train_samples_per_second': 12.177, 'train_steps_per_second': 0.095, 'train_loss': 0.08067458087636023, 'epoch': 1.0}
***** train metrics *****
  epoch                    =      0.9992
  total_flos               = 441020272GF
  train_loss               =      0.0807
  train_runtime            =  1:25:01.96
  train_samples_per_second =      12.177
  train_steps_per_second   =       0.095
Figure saved at: saves/llama3_lora_sft_random_all_totally/training_loss.png
10/07/2024 23:34:15 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
10/07/2024 23:34:15 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|modelcard.py:449] 2024-10-07 23:34:15,871 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 🚀 View run [33msaves/llama3_lora_sft_random_all_totally[0m at: [34mhttps://wandb.ai/beiweixiaoxu/huggingface/runs/hyk15ws4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241007_220915-hyk15ws4/logs[0m
[2024-10-07 23:34:26,916] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
10/07/2024 23:34:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:731] 2024-10-07 23:34:30,941 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 23:34:30,943 >> Model config LlamaConfig {
  "_name_or_path": "/home/sth/data/code/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:30,944 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:30,944 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:30,944 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:30,944 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2024-10-07 23:34:31,262 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:731] 2024-10-07 23:34:31,265 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 23:34:31,268 >> Model config LlamaConfig {
  "_name_or_path": "/home/sth/data/code/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:31,271 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:31,271 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:31,271 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2267] 2024-10-07 23:34:31,271 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2513] 2024-10-07 23:34:31,584 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
10/07/2024 23:34:31 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>
10/07/2024 23:34:31 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>
10/07/2024 23:34:31 - INFO - llamafactory.data.loader - Loading dataset cross_culture_random_all_totally.json...
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 77086 examples [00:00, 134012.98 examples/s]Generating train split: 77086 examples [00:00, 133941.75 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/77086 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 2485/77086 [00:00<00:03, 24806.91 examples/s]Converting format of dataset (num_proc=16):  31%|███       | 23718/77086 [00:00<00:00, 130777.52 examples/s]Converting format of dataset (num_proc=16):  62%|██████▏   | 47684/77086 [00:00<00:00, 179006.94 examples/s]Converting format of dataset (num_proc=16):  91%|█████████▏| 70346/77086 [00:00<00:00, 195850.82 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 77086/77086 [00:00<00:00, 133618.10 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/77086 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   1%|▏         | 1000/77086 [00:00<01:11, 1070.25 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 2000/77086 [00:01<00:34, 2157.32 examples/s]Running tokenizer on dataset (num_proc=16):   4%|▍         | 3000/77086 [00:01<00:23, 3181.78 examples/s]Running tokenizer on dataset (num_proc=16):   5%|▌         | 4000/77086 [00:01<00:17, 4099.51 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 5000/77086 [00:01<00:14, 4863.36 examples/s]Running tokenizer on dataset (num_proc=16):   9%|▉         | 7000/77086 [00:01<00:09, 7323.29 examples/s]Running tokenizer on dataset (num_proc=16):  12%|█▏        | 9000/77086 [00:01<00:07, 9174.54 examples/s]Running tokenizer on dataset (num_proc=16):  14%|█▍        | 11000/77086 [00:01<00:06, 10559.81 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 15000/77086 [00:02<00:04, 15278.05 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 18000/77086 [00:02<00:03, 16388.67 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███       | 24000/77086 [00:02<00:02, 25764.75 examples/s]Running tokenizer on dataset (num_proc=16):  35%|███▌      | 27000/77086 [00:02<00:01, 26521.80 examples/s]Running tokenizer on dataset (num_proc=16):  39%|███▉      | 30000/77086 [00:02<00:01, 25462.70 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 33818/77086 [00:02<00:01, 27175.23 examples/s]Running tokenizer on dataset (num_proc=16):  49%|████▉     | 37818/77086 [00:02<00:01, 30271.59 examples/s]Running tokenizer on dataset (num_proc=16):  55%|█████▌    | 42636/77086 [00:02<00:01, 32065.44 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 46454/77086 [00:03<00:00, 32395.66 examples/s]Running tokenizer on dataset (num_proc=16):  65%|██████▌   | 50272/77086 [00:03<00:00, 30993.32 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 54090/77086 [00:03<00:00, 28706.79 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 57908/77086 [00:03<00:00, 28164.74 examples/s]Running tokenizer on dataset (num_proc=16):  80%|███████▉  | 61544/77086 [00:03<00:00, 27626.41 examples/s]Running tokenizer on dataset (num_proc=16):  83%|████████▎ | 64362/77086 [00:03<00:00, 25754.64 examples/s]Running tokenizer on dataset (num_proc=16):  87%|████████▋ | 67180/77086 [00:03<00:00, 24383.98 examples/s]Running tokenizer on dataset (num_proc=16):  91%|█████████ | 69998/77086 [00:04<00:00, 21958.60 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 72816/77086 [00:04<00:00, 18449.39 examples/s]Running tokenizer on dataset (num_proc=16):  98%|█████████▊| 75452/77086 [00:04<00:00, 16565.70 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 77086/77086 [00:04<00:00, 15882.06 examples/s]
[INFO|configuration_utils.py:731] 2024-10-07 23:34:40,094 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-07 23:34:40,095 >> Model config LlamaConfig {
  "_name_or_path": "/home/sth/data/code/Meta-Llama-3-8B-Instruct",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3675] 2024-10-07 23:34:40,122 >> loading weights file /home/sth/data/code/Meta-Llama-3-8B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1606] 2024-10-07 23:34:40,122 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1038] 2024-10-07 23:34:40,124 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

training example:
input_ids:
[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 1972, 1732, 449, 459, 3778, 13042, 4092, 13, 5321, 5266, 704, 279, 4435, 26028, 24507, 323, 4320, 279, 4860, 27136, 4184, 311, 701, 1866, 907, 1887, 13, 128009, 128006, 882, 128007, 271, 22818, 264, 674, 14924, 323, 674, 3883, 11, 5268, 279, 3072, 430, 1888, 5398, 82, 449, 701, 1866, 907, 1887, 311, 4320, 279, 3488, 627, 2, 14924, 25, 2057, 1148, 13112, 656, 499, 7655, 449, 279, 5224, 25, 364, 40, 1053, 2733, 10882, 19261, 704, 311, 264, 9760, 477, 82407, 369, 1520, 449, 264, 3465, 477, 1886, 438, 6, 5380, 2, 3883, 25, 220, 16, 885, 2294, 3568, 220, 17, 35007, 635, 264, 2763, 220, 18, 15350, 1633, 1790, 220, 19, 18982, 520, 682, 198, 5618, 471, 279, 1396, 315, 279, 4183, 3072, 1193, 13, 128009, 128006, 78191, 128007, 271, 17, 128009]
inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a real person with an American cultural background. Please fill out the World Values Survey and answer the questions honestly according to your own value system.<|eot_id|><|start_header_id|>user<|end_header_id|>

Given a #Question and #Options, choose the option that best aligns with your own value system to answer the question.
#Question: To what extent do you agree with the statement: 'I would feel comfortable reaching out to a neighbor or acquaintance for help with a task or errand'?
#Options: 1.A great deal 2.Quite a lot 3.Not very much 4.None at all
Please return the number of the selected option only.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

2<|eot_id|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 17, 128009]
labels:
2<|eot_id|>
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
[INFO|modeling_utils.py:4507] 2024-10-07 23:34:44,920 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4515] 2024-10-07 23:34:44,920 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/sth/data/code/Meta-Llama-3-8B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:991] 2024-10-07 23:34:44,922 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/generation_config.json
[INFO|configuration_utils.py:1038] 2024-10-07 23:34:44,922 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128009
  ],
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}

10/07/2024 23:34:44 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
10/07/2024 23:34:44 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
10/07/2024 23:34:44 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
10/07/2024 23:34:44 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
10/07/2024 23:34:44 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,down_proj,q_proj,up_proj,gate_proj,k_proj,o_proj
10/07/2024 23:34:45 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:648] 2024-10-07 23:34:45,576 >> Using auto half precision backend
[INFO|trainer.py:2134] 2024-10-07 23:34:45,868 >> ***** Running training *****
[INFO|trainer.py:2135] 2024-10-07 23:34:45,868 >>   Num examples = 77,086
[INFO|trainer.py:2136] 2024-10-07 23:34:45,868 >>   Num Epochs = 1
[INFO|trainer.py:2137] 2024-10-07 23:34:45,868 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2140] 2024-10-07 23:34:45,868 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2141] 2024-10-07 23:34:45,868 >>   Gradient Accumulation steps = 32
[INFO|trainer.py:2142] 2024-10-07 23:34:45,868 >>   Total optimization steps = 602
[INFO|trainer.py:2143] 2024-10-07 23:34:45,872 >>   Number of trainable parameters = 20,971,520
[INFO|integration_utils.py:807] 2024-10-07 23:34:45,880 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: beiweixiaoxu. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.18.0
wandb: Run data is saved locally in /data/syxu/culture_steering/LLaMA-Factory/wandb/run-20241007_233447-5aebjmc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run saves/llama3_lora_sft_random_all_cross_totally
wandb: ⭐️ View project at https://wandb.ai/beiweixiaoxu/huggingface
wandb: 🚀 View run at https://wandb.ai/beiweixiaoxu/huggingface/runs/5aebjmc3
  0%|          | 0/602 [00:00<?, ?it/s]  0%|          | 1/602 [00:11<1:50:18, 11.01s/it]  0%|          | 2/602 [00:21<1:45:37, 10.56s/it]  0%|          | 3/602 [00:31<1:44:42, 10.49s/it]  1%|          | 4/602 [00:42<1:44:36, 10.50s/it]  1%|          | 5/602 [00:52<1:44:08, 10.47s/it]  1%|          | 6/602 [01:02<1:43:22, 10.41s/it]  1%|          | 7/602 [01:13<1:43:34, 10.44s/it]  1%|▏         | 8/602 [01:23<1:43:36, 10.47s/it]  1%|▏         | 9/602 [01:34<1:43:15, 10.45s/it]  2%|▏         | 10/602 [01:44<1:42:19, 10.37s/it]                                                    2%|▏         | 10/602 [01:44<1:42:19, 10.37s/it]  2%|▏         | 11/602 [01:54<1:42:22, 10.39s/it]  2%|▏         | 12/602 [02:05<1:42:46, 10.45s/it]  2%|▏         | 13/602 [02:15<1:42:34, 10.45s/it]  2%|▏         | 14/602 [02:26<1:42:16, 10.44s/it]  2%|▏         | 15/602 [02:37<1:42:59, 10.53s/it]  3%|▎         | 16/602 [02:47<1:42:58, 10.54s/it]  3%|▎         | 17/602 [02:58<1:42:33, 10.52s/it]  3%|▎         | 18/602 [03:09<1:43:18, 10.61s/it]  3%|▎         | 19/602 [03:19<1:42:23, 10.54s/it]  3%|▎         | 20/602 [03:29<1:41:42, 10.49s/it]                                                    3%|▎         | 20/602 [03:29<1:41:42, 10.49s/it]  3%|▎         | 21/602 [03:40<1:41:24, 10.47s/it]  4%|▎         | 22/602 [03:50<1:40:41, 10.42s/it]  4%|▍         | 23/602 [04:01<1:41:25, 10.51s/it]  4%|▍         | 24/602 [04:12<1:42:11, 10.61s/it]  4%|▍         | 25/602 [04:22<1:41:23, 10.54s/it]  4%|▍         | 26/602 [04:32<1:41:08, 10.54s/it]  4%|▍         | 27/602 [04:43<1:41:50, 10.63s/it]  5%|▍         | 28/602 [04:54<1:41:19, 10.59s/it]  5%|▍         | 29/602 [05:04<1:40:50, 10.56s/it]  5%|▍         | 30/602 [05:15<1:40:03, 10.50s/it]                                                    5%|▍         | 30/602 [05:15<1:40:03, 10.50s/it]  5%|▌         | 31/602 [05:25<1:40:58, 10.61s/it]  5%|▌         | 32/602 [05:36<1:40:17, 10.56s/it]  5%|▌         | 33/602 [05:46<1:39:51, 10.53s/it]  6%|▌         | 34/602 [05:57<1:39:43, 10.53s/it]  6%|▌         | 35/602 [06:07<1:39:23, 10.52s/it]  6%|▌         | 36/602 [06:18<1:39:29, 10.55s/it]  6%|▌         | 37/602 [06:28<1:38:34, 10.47s/it]  6%|▋         | 38/602 [06:39<1:38:42, 10.50s/it]  6%|▋         | 39/602 [06:49<1:38:34, 10.51s/it]  7%|▋         | 40/602 [07:00<1:38:50, 10.55s/it]                                                    7%|▋         | 40/602 [07:00<1:38:50, 10.55s/it]  7%|▋         | 41/602 [07:11<1:38:43, 10.56s/it]  7%|▋         | 42/602 [07:21<1:38:03, 10.51s/it]  7%|▋         | 43/602 [07:31<1:37:26, 10.46s/it]  7%|▋         | 44/602 [07:42<1:37:42, 10.51s/it]  7%|▋         | 45/602 [07:53<1:37:50, 10.54s/it]  8%|▊         | 46/602 [08:03<1:37:33, 10.53s/it]  8%|▊         | 47/602 [08:14<1:37:06, 10.50s/it]  8%|▊         | 48/602 [08:24<1:36:44, 10.48s/it]  8%|▊         | 49/602 [08:34<1:36:00, 10.42s/it]  8%|▊         | 50/602 [08:45<1:35:55, 10.43s/it]                                                    8%|▊         | 50/602 [08:45<1:35:55, 10.43s/it]  8%|▊         | 51/602 [08:55<1:35:28, 10.40s/it]  9%|▊         | 52/602 [09:06<1:35:58, 10.47s/it]  9%|▉         | 53/602 [09:16<1:35:57, 10.49s/it]  9%|▉         | 54/602 [09:27<1:35:37, 10.47s/it]  9%|▉         | 55/602 [09:37<1:35:48, 10.51s/it]  9%|▉         | 56/602 [09:48<1:36:28, 10.60s/it]  9%|▉         | 57/602 [09:59<1:36:14, 10.59s/it] 10%|▉         | 58/602 [10:09<1:36:07, 10.60s/it] 10%|▉         | 59/602 [10:20<1:36:01, 10.61s/it] 10%|▉         | 60/602 [10:30<1:35:11, 10.54s/it]                                                   10%|▉         | 60/602 [10:30<1:35:11, 10.54s/it] 10%|█         | 61/602 [10:41<1:35:10, 10.56s/it] 10%|█         | 62/602 [10:51<1:35:00, 10.56s/it] 10%|█         | 63/602 [11:02<1:34:54, 10.56s/it] 11%|█         | 64/602 [11:13<1:34:38, 10.56s/it] 11%|█         | 65/602 [11:23<1:34:31, 10.56s/it] 11%|█         | 66/602 [11:33<1:33:49, 10.50s/it] 11%|█         | 67/602 [11:44<1:34:41, 10.62s/it] 11%|█▏        | 68/602 [11:55<1:34:13, 10.59s/it] 11%|█▏        | 69/602 [12:05<1:34:04, 10.59s/it] 12%|█▏        | 70/602 [12:16<1:33:53, 10.59s/it]                                                   12%|█▏        | 70/602 [12:16<1:33:53, 10.59s/it] 12%|█▏        | 71/602 [12:26<1:33:20, 10.55s/it] 12%|█▏        | 72/602 [12:37<1:32:59, 10.53s/it] 12%|█▏        | 73/602 [12:47<1:32:41, 10.51s/it] 12%|█▏        | 74/602 [12:58<1:31:49, 10.43s/it] 12%|█▏        | 75/602 [13:08<1:31:43, 10.44s/it] 13%|█▎        | 76/602 [13:19<1:32:03, 10.50s/it] 13%|█▎        | 77/602 [13:29<1:31:52, 10.50s/it] 13%|█▎        | 78/602 [13:40<1:31:38, 10.49s/it] 13%|█▎        | 79/602 [13:50<1:31:17, 10.47s/it] 13%|█▎        | 80/602 [14:01<1:31:43, 10.54s/it]                                                   13%|█▎        | 80/602 [14:01<1:31:43, 10.54s/it] 13%|█▎        | 81/602 [14:11<1:31:24, 10.53s/it] 14%|█▎        | 82/602 [14:22<1:31:09, 10.52s/it] 14%|█▍        | 83/602 [14:32<1:30:32, 10.47s/it] 14%|█▍        | 84/602 [14:43<1:30:08, 10.44s/it] 14%|█▍        | 85/602 [14:53<1:29:46, 10.42s/it] 14%|█▍        | 86/602 [15:03<1:29:02, 10.35s/it] 14%|█▍        | 87/602 [15:14<1:28:46, 10.34s/it] 15%|█▍        | 88/602 [15:24<1:28:43, 10.36s/it] 15%|█▍        | 89/602 [15:34<1:28:46, 10.38s/it] 15%|█▍        | 90/602 [15:45<1:29:24, 10.48s/it]                                                   15%|█▍        | 90/602 [15:45<1:29:24, 10.48s/it] 15%|█▌        | 91/602 [15:56<1:29:31, 10.51s/it] 15%|█▌        | 92/602 [16:06<1:29:19, 10.51s/it] 15%|█▌        | 93/602 [16:17<1:29:14, 10.52s/it] 16%|█▌        | 94/602 [16:27<1:29:27, 10.57s/it] 16%|█▌        | 95/602 [16:38<1:29:39, 10.61s/it] 16%|█▌        | 96/602 [16:49<1:29:12, 10.58s/it] 16%|█▌        | 97/602 [16:59<1:28:48, 10.55s/it] 16%|█▋        | 98/602 [17:09<1:28:20, 10.52s/it] 16%|█▋        | 99/602 [17:20<1:27:59, 10.50s/it] 17%|█▋        | 100/602 [17:30<1:27:17, 10.43s/it]                                                    17%|█▋        | 100/602 [17:30<1:27:17, 10.43s/it] 17%|█▋        | 101/602 [17:40<1:26:41, 10.38s/it] 17%|█▋        | 102/602 [17:51<1:27:03, 10.45s/it] 17%|█▋        | 103/602 [18:02<1:27:02, 10.47s/it] 17%|█▋        | 104/602 [18:12<1:27:07, 10.50s/it] 17%|█▋        | 105/602 [18:23<1:27:20, 10.54s/it] 18%|█▊        | 106/602 [18:33<1:26:25, 10.45s/it] 18%|█▊        | 107/602 [18:43<1:26:00, 10.43s/it] 18%|█▊        | 108/602 [18:54<1:26:35, 10.52s/it] 18%|█▊        | 109/602 [19:05<1:26:01, 10.47s/it] 18%|█▊        | 110/602 [19:15<1:25:26, 10.42s/it]                                                    18%|█▊        | 110/602 [19:15<1:25:26, 10.42s/it] 18%|█▊        | 111/602 [19:25<1:25:02, 10.39s/it] 19%|█▊        | 112/602 [19:36<1:24:58, 10.41s/it] 19%|█▉        | 113/602 [19:46<1:24:39, 10.39s/it] 19%|█▉        | 114/602 [19:56<1:24:46, 10.42s/it] 19%|█▉        | 115/602 [20:07<1:24:21, 10.39s/it] 19%|█▉        | 116/602 [20:17<1:24:09, 10.39s/it] 19%|█▉        | 117/602 [20:28<1:24:49, 10.49s/it] 20%|█▉        | 118/602 [20:39<1:25:24, 10.59s/it] 20%|█▉        | 119/602 [20:49<1:24:30, 10.50s/it] 20%|█▉        | 120/602 [20:59<1:24:13, 10.49s/it]                                                    20%|█▉        | 120/602 [20:59<1:24:13, 10.49s/it] 20%|██        | 121/602 [21:10<1:24:15, 10.51s/it] 20%|██        | 122/602 [21:20<1:23:20, 10.42s/it] 20%|██        | 123/602 [21:31<1:23:20, 10.44s/it] 21%|██        | 124/602 [21:41<1:23:08, 10.44s/it] 21%|██        | 125/602 [21:52<1:22:57, 10.44s/it] 21%|██        | 126/602 [22:02<1:22:15, 10.37s/it] 21%|██        | 127/602 [22:12<1:21:57, 10.35s/it] 21%|██▏       | 128/602 [22:23<1:23:05, 10.52s/it] 21%|██▏       | 129/602 [22:33<1:22:42, 10.49s/it] 22%|██▏       | 130/602 [22:44<1:22:51, 10.53s/it]                                                    22%|██▏       | 130/602 [22:44<1:22:51, 10.53s/it] 22%|██▏       | 131/602 [22:54<1:22:18, 10.48s/it] 22%|██▏       | 132/602 [23:05<1:22:33, 10.54s/it] 22%|██▏       | 133/602 [23:15<1:21:58, 10.49s/it] 22%|██▏       | 134/602 [23:26<1:20:59, 10.38s/it] 22%|██▏       | 135/602 [23:36<1:21:36, 10.49s/it] 23%|██▎       | 136/602 [23:47<1:21:37, 10.51s/it] 23%|██▎       | 137/602 [23:57<1:21:16, 10.49s/it] 23%|██▎       | 138/602 [24:08<1:21:02, 10.48s/it] 23%|██▎       | 139/602 [24:18<1:21:01, 10.50s/it] 23%|██▎       | 140/602 [24:29<1:21:10, 10.54s/it]                                                    23%|██▎       | 140/602 [24:29<1:21:10, 10.54s/it] 23%|██▎       | 141/602 [24:39<1:20:48, 10.52s/it] 24%|██▎       | 142/602 [24:50<1:20:07, 10.45s/it] 24%|██▍       | 143/602 [25:00<1:19:56, 10.45s/it] 24%|██▍       | 144/602 [25:11<1:19:38, 10.43s/it] 24%|██▍       | 145/602 [25:21<1:19:18, 10.41s/it] 24%|██▍       | 146/602 [25:31<1:19:04, 10.40s/it] 24%|██▍       | 147/602 [25:42<1:18:52, 10.40s/it] 25%|██▍       | 148/602 [25:52<1:18:43, 10.40s/it] 25%|██▍       | 149/602 [26:03<1:18:39, 10.42s/it] 25%|██▍       | 150/602 [26:13<1:18:20, 10.40s/it]                                                    25%|██▍       | 150/602 [26:13<1:18:20, 10.40s/it] 25%|██▌       | 151/602 [26:23<1:18:21, 10.42s/it] 25%|██▌       | 152/602 [26:34<1:18:21, 10.45s/it] 25%|██▌       | 153/602 [26:44<1:17:59, 10.42s/it] 26%|██▌       | 154/602 [26:55<1:17:31, 10.38s/it] 26%|██▌       | 155/602 [27:05<1:17:18, 10.38s/it] 26%|██▌       | 156/602 [27:15<1:17:31, 10.43s/it] 26%|██▌       | 157/602 [27:26<1:17:23, 10.44s/it] 26%|██▌       | 158/602 [27:36<1:16:58, 10.40s/it] 26%|██▋       | 159/602 [27:47<1:17:24, 10.48s/it] 27%|██▋       | 160/602 [27:57<1:17:13, 10.48s/it]                                                    27%|██▋       | 160/602 [27:57<1:17:13, 10.48s/it] 27%|██▋       | 161/602 [28:08<1:16:34, 10.42s/it] 27%|██▋       | 162/602 [28:18<1:16:50, 10.48s/it] 27%|██▋       | 163/602 [28:29<1:17:11, 10.55s/it] 27%|██▋       | 164/602 [28:40<1:17:03, 10.56s/it] 27%|██▋       | 165/602 [28:50<1:17:00, 10.57s/it] 28%|██▊       | 166/602 [29:01<1:16:27, 10.52s/it] 28%|██▊       | 167/602 [29:11<1:15:56, 10.47s/it] 28%|██▊       | 168/602 [29:21<1:15:48, 10.48s/it] 28%|██▊       | 169/602 [29:32<1:15:44, 10.49s/it] 28%|██▊       | 170/602 [29:42<1:15:11, 10.44s/it]                                                    28%|██▊       | 170/602 [29:42<1:15:11, 10.44s/it] 28%|██▊       | 171/602 [29:53<1:15:16, 10.48s/it] 29%|██▊       | 172/602 [30:03<1:15:07, 10.48s/it] 29%|██▊       | 173/602 [30:14<1:14:55, 10.48s/it] 29%|██▉       | 174/602 [30:24<1:14:40, 10.47s/it] 29%|██▉       | 175/602 [30:35<1:14:20, 10.45s/it] 29%|██▉       | 176/602 [30:45<1:14:17, 10.46s/it] 29%|██▉       | 177/602 [30:56<1:14:32, 10.52s/it] 30%|██▉       | 178/602 [31:06<1:14:25, 10.53s/it] 30%|██▉       | 179/602 [31:17<1:13:29, 10.43s/it] 30%|██▉       | 180/602 [31:27<1:13:11, 10.41s/it]                                                    30%|██▉       | 180/602 [31:27<1:13:11, 10.41s/it] 30%|███       | 181/602 [31:37<1:13:09, 10.43s/it] 30%|███       | 182/602 [31:48<1:12:46, 10.40s/it] 30%|███       | 183/602 [31:58<1:12:33, 10.39s/it] 31%|███       | 184/602 [32:09<1:12:35, 10.42s/it] 31%|███       | 185/602 [32:19<1:12:55, 10.49s/it] 31%|███       | 186/602 [32:30<1:12:49, 10.50s/it] 31%|███       | 187/602 [32:40<1:12:51, 10.53s/it] 31%|███       | 188/602 [32:51<1:12:22, 10.49s/it] 31%|███▏      | 189/602 [33:01<1:11:49, 10.43s/it] 32%|███▏      | 190/602 [33:11<1:11:25, 10.40s/it]                                                    32%|███▏      | 190/602 [33:11<1:11:25, 10.40s/it] 32%|███▏      | 191/602 [33:22<1:11:13, 10.40s/it] 32%|███▏      | 192/602 [33:32<1:11:30, 10.47s/it] 32%|███▏      | 193/602 [33:43<1:11:12, 10.45s/it] 32%|███▏      | 194/602 [33:53<1:11:11, 10.47s/it] 32%|███▏      | 195/602 [34:04<1:11:41, 10.57s/it] 33%|███▎      | 196/602 [34:15<1:11:28, 10.56s/it] 33%|███▎      | 197/602 [34:25<1:10:59, 10.52s/it] 33%|███▎      | 198/602 [34:36<1:10:53, 10.53s/it] 33%|███▎      | 199/602 [34:46<1:10:36, 10.51s/it] 33%|███▎      | 200/602 [34:57<1:10:19, 10.50s/it]                                                    33%|███▎      | 200/602 [34:57<1:10:19, 10.50s/it] 33%|███▎      | 201/602 [35:07<1:10:02, 10.48s/it] 34%|███▎      | 202/602 [35:17<1:09:48, 10.47s/it] 34%|███▎      | 203/602 [35:28<1:09:49, 10.50s/it] 34%|███▍      | 204/602 [35:38<1:09:05, 10.42s/it] 34%|███▍      | 205/602 [35:49<1:09:11, 10.46s/it] 34%|███▍      | 206/602 [35:59<1:08:56, 10.44s/it] 34%|███▍      | 207/602 [36:10<1:09:05, 10.50s/it] 35%|███▍      | 208/602 [36:20<1:08:59, 10.51s/it] 35%|███▍      | 209/602 [36:31<1:08:27, 10.45s/it] 35%|███▍      | 210/602 [36:41<1:08:35, 10.50s/it]                                                    35%|███▍      | 210/602 [36:41<1:08:35, 10.50s/it] 35%|███▌      | 211/602 [36:52<1:08:25, 10.50s/it] 35%|███▌      | 212/602 [37:02<1:08:13, 10.50s/it] 35%|███▌      | 213/602 [37:13<1:07:33, 10.42s/it] 36%|███▌      | 214/602 [37:23<1:07:54, 10.50s/it] 36%|███▌      | 215/602 [37:34<1:07:26, 10.46s/it] 36%|███▌      | 216/602 [37:44<1:07:48, 10.54s/it] 36%|███▌      | 217/602 [37:55<1:07:26, 10.51s/it] 36%|███▌      | 218/602 [38:05<1:07:24, 10.53s/it] 36%|███▋      | 219/602 [38:16<1:07:02, 10.50s/it] 37%|███▋      | 220/602 [38:26<1:06:27, 10.44s/it]                                                    37%|███▋      | 220/602 [38:26<1:06:27, 10.44s/it] 37%|███▋      | 221/602 [38:37<1:06:25, 10.46s/it] 37%|███▋      | 222/602 [38:47<1:06:11, 10.45s/it] 37%|███▋      | 223/602 [38:57<1:05:22, 10.35s/it] 37%|███▋      | 224/602 [39:07<1:05:14, 10.36s/it] 37%|███▋      | 225/602 [39:18<1:05:42, 10.46s/it] 38%|███▊      | 226/602 [39:29<1:05:50, 10.51s/it] 38%|███▊      | 227/602 [39:39<1:05:56, 10.55s/it] 38%|███▊      | 228/602 [39:50<1:05:04, 10.44s/it] 38%|███▊      | 229/602 [40:00<1:04:46, 10.42s/it] 38%|███▊      | 230/602 [40:10<1:04:40, 10.43s/it]                                                    38%|███▊      | 230/602 [40:10<1:04:40, 10.43s/it] 38%|███▊      | 231/602 [40:21<1:04:35, 10.45s/it] 39%|███▊      | 232/602 [40:31<1:04:34, 10.47s/it] 39%|███▊      | 233/602 [40:42<1:04:18, 10.46s/it] 39%|███▉      | 234/602 [40:52<1:04:09, 10.46s/it] 39%|███▉      | 235/602 [41:03<1:04:22, 10.53s/it] 39%|███▉      | 236/602 [41:14<1:04:07, 10.51s/it] 39%|███▉      | 237/602 [41:24<1:03:57, 10.51s/it] 40%|███▉      | 238/602 [41:35<1:03:53, 10.53s/it] 40%|███▉      | 239/602 [41:45<1:03:23, 10.48s/it] 40%|███▉      | 240/602 [41:56<1:03:56, 10.60s/it]                                                    40%|███▉      | 240/602 [41:56<1:03:56, 10.60s/it] 40%|████      | 241/602 [42:07<1:04:13, 10.68s/it] 40%|████      | 242/602 [42:17<1:03:44, 10.62s/it] 40%|████      | 243/602 [42:28<1:03:11, 10.56s/it] 41%|████      | 244/602 [42:38<1:02:35, 10.49s/it] 41%|████      | 245/602 [42:48<1:02:24, 10.49s/it] 41%|████      | 246/602 [42:59<1:02:20, 10.51s/it] 41%|████      | 247/602 [43:09<1:02:05, 10.49s/it] 41%|████      | 248/602 [43:20<1:01:23, 10.41s/it] 41%|████▏     | 249/602 [43:30<1:01:41, 10.49s/it] 42%|████▏     | 250/602 [43:41<1:01:11, 10.43s/it]                                                    42%|████▏     | 250/602 [43:41<1:01:11, 10.43s/it] 42%|████▏     | 251/602 [43:51<1:00:49, 10.40s/it] 42%|████▏     | 252/602 [44:01<1:00:28, 10.37s/it] 42%|████▏     | 253/602 [44:12<1:00:41, 10.43s/it] 42%|████▏     | 254/602 [44:22<1:00:37, 10.45s/it] 42%|████▏     | 255/602 [44:33<1:00:35, 10.48s/it] 43%|████▎     | 256/602 [44:43<1:00:31, 10.50s/it] 43%|████▎     | 257/602 [44:54<1:00:13, 10.48s/it] 43%|████▎     | 258/602 [45:04<1:00:06, 10.48s/it] 43%|████▎     | 259/602 [45:15<1:00:22, 10.56s/it] 43%|████▎     | 260/602 [45:25<59:51, 10.50s/it]                                                    43%|████▎     | 260/602 [45:25<59:51, 10.50s/it] 43%|████▎     | 261/602 [45:36<59:37, 10.49s/it] 44%|████▎     | 262/602 [45:46<59:12, 10.45s/it] 44%|████▎     | 263/602 [45:57<59:10, 10.47s/it] 44%|████▍     | 264/602 [46:07<58:28, 10.38s/it] 44%|████▍     | 265/602 [46:18<58:55, 10.49s/it] 44%|████▍     | 266/602 [46:28<58:31, 10.45s/it] 44%|████▍     | 267/602 [46:39<58:48, 10.53s/it] 45%|████▍     | 268/602 [46:49<58:40, 10.54s/it] 45%|████▍     | 269/602 [47:00<58:43, 10.58s/it] 45%|████▍     | 270/602 [47:11<58:33, 10.58s/it]                                                  45%|████▍     | 270/602 [47:11<58:33, 10.58s/it] 45%|████▌     | 271/602 [47:21<58:29, 10.60s/it] 45%|████▌     | 272/602 [47:31<57:41, 10.49s/it] 45%|████▌     | 273/602 [47:42<57:20, 10.46s/it] 46%|████▌     | 274/602 [47:52<56:58, 10.42s/it] 46%|████▌     | 275/602 [48:03<56:50, 10.43s/it] 46%|████▌     | 276/602 [48:13<57:00, 10.49s/it] 46%|████▌     | 277/602 [48:24<57:25, 10.60s/it] 46%|████▌     | 278/602 [48:35<57:14, 10.60s/it] 46%|████▋     | 279/602 [48:45<56:48, 10.55s/it] 47%|████▋     | 280/602 [48:56<56:46, 10.58s/it]                                                  47%|████▋     | 280/602 [48:56<56:46, 10.58s/it] 47%|████▋     | 281/602 [49:06<56:34, 10.58s/it] 47%|████▋     | 282/602 [49:17<56:28, 10.59s/it] 47%|████▋     | 283/602 [49:27<56:03, 10.54s/it] 47%|████▋     | 284/602 [49:38<55:57, 10.56s/it] 47%|████▋     | 285/602 [49:49<55:43, 10.55s/it] 48%|████▊     | 286/602 [49:59<56:10, 10.66s/it] 48%|████▊     | 287/602 [50:10<56:20, 10.73s/it] 48%|████▊     | 288/602 [50:21<55:51, 10.67s/it] 48%|████▊     | 289/602 [50:31<55:09, 10.57s/it] 48%|████▊     | 290/602 [50:42<54:48, 10.54s/it]                                                  48%|████▊     | 290/602 [50:42<54:48, 10.54s/it] 48%|████▊     | 291/602 [50:52<54:31, 10.52s/it] 49%|████▊     | 292/602 [51:03<54:17, 10.51s/it] 49%|████▊     | 293/602 [51:13<53:55, 10.47s/it] 49%|████▉     | 294/602 [51:23<53:19, 10.39s/it] 49%|████▉     | 295/602 [51:34<52:56, 10.35s/it] 49%|████▉     | 296/602 [51:44<52:31, 10.30s/it] 49%|████▉     | 297/602 [51:54<52:43, 10.37s/it] 50%|████▉     | 298/602 [52:05<52:56, 10.45s/it] 50%|████▉     | 299/602 [52:15<52:32, 10.40s/it] 50%|████▉     | 300/602 [52:26<52:20, 10.40s/it]                                                  50%|████▉     | 300/602 [52:26<52:20, 10.40s/it] 50%|█████     | 301/602 [52:36<52:30, 10.47s/it] 50%|█████     | 302/602 [52:47<52:16, 10.45s/it] 50%|█████     | 303/602 [52:57<52:25, 10.52s/it] 50%|█████     | 304/602 [53:08<52:09, 10.50s/it] 51%|█████     | 305/602 [53:30<1:10:02, 14.15s/it] 51%|█████     | 306/602 [53:56<1:27:10, 17.67s/it] 51%|█████     | 307/602 [54:21<1:37:35, 19.85s/it] 51%|█████     | 308/602 [54:46<1:45:07, 21.45s/it] 51%|█████▏    | 309/602 [55:03<1:37:56, 20.05s/it] 51%|█████▏    | 310/602 [55:14<1:23:43, 17.21s/it]                                                    51%|█████▏    | 310/602 [55:14<1:23:43, 17.21s/it] 52%|█████▏    | 311/602 [55:24<1:13:44, 15.20s/it] 52%|█████▏    | 312/602 [55:35<1:06:33, 13.77s/it] 52%|█████▏    | 313/602 [55:45<1:01:36, 12.79s/it] 52%|█████▏    | 314/602 [55:55<57:45, 12.03s/it]   52%|█████▏    | 315/602 [56:06<55:06, 11.52s/it] 52%|█████▏    | 316/602 [56:17<53:50, 11.30s/it] 53%|█████▎    | 317/602 [56:27<52:08, 10.98s/it] 53%|█████▎    | 318/602 [56:37<51:05, 10.80s/it] 53%|█████▎    | 319/602 [56:48<50:38, 10.74s/it] 53%|█████▎    | 320/602 [56:58<49:52, 10.61s/it]                                                  53%|█████▎    | 320/602 [56:58<49:52, 10.61s/it] 53%|█████▎    | 321/602 [57:09<49:34, 10.59s/it] 53%|█████▎    | 322/602 [57:19<49:17, 10.56s/it] 54%|█████▎    | 323/602 [57:30<49:04, 10.56s/it] 54%|█████▍    | 324/602 [57:40<48:55, 10.56s/it] 54%|█████▍    | 325/602 [57:51<48:51, 10.58s/it] 54%|█████▍    | 326/602 [58:01<48:27, 10.54s/it] 54%|█████▍    | 327/602 [58:12<48:16, 10.53s/it] 54%|█████▍    | 328/602 [58:22<47:45, 10.46s/it] 55%|█████▍    | 329/602 [58:33<47:59, 10.55s/it] 55%|█████▍    | 330/602 [58:44<47:58, 10.58s/it]                                                  55%|█████▍    | 330/602 [58:44<47:58, 10.58s/it] 55%|█████▍    | 331/602 [58:54<47:23, 10.49s/it] 55%|█████▌    | 332/602 [59:04<47:24, 10.53s/it] 55%|█████▌    | 333/602 [59:15<47:27, 10.59s/it] 55%|█████▌    | 334/602 [59:26<47:21, 10.60s/it] 56%|█████▌    | 335/602 [59:36<47:10, 10.60s/it] 56%|█████▌    | 336/602 [59:47<46:43, 10.54s/it] 56%|█████▌    | 337/602 [59:57<46:22, 10.50s/it] 56%|█████▌    | 338/602 [1:00:08<45:58, 10.45s/it] 56%|█████▋    | 339/602 [1:00:18<46:15, 10.55s/it] 56%|█████▋    | 340/602 [1:00:29<46:11, 10.58s/it]                                                    56%|█████▋    | 340/602 [1:00:29<46:11, 10.58s/it] 57%|█████▋    | 341/602 [1:00:39<45:55, 10.56s/it] 57%|█████▋    | 342/602 [1:00:50<45:47, 10.57s/it] 57%|█████▋    | 343/602 [1:01:00<45:21, 10.51s/it] 57%|█████▋    | 344/602 [1:01:11<44:56, 10.45s/it] 57%|█████▋    | 345/602 [1:01:21<44:47, 10.46s/it] 57%|█████▋    | 346/602 [1:01:32<44:37, 10.46s/it] 58%|█████▊    | 347/602 [1:01:42<44:28, 10.47s/it] 58%|█████▊    | 348/602 [1:01:53<44:09, 10.43s/it] 58%|█████▊    | 349/602 [1:02:03<43:54, 10.41s/it] 58%|█████▊    | 350/602 [1:02:13<43:44, 10.42s/it]                                                    58%|█████▊    | 350/602 [1:02:13<43:44, 10.42s/it] 58%|█████▊    | 351/602 [1:02:24<43:51, 10.48s/it] 58%|█████▊    | 352/602 [1:02:34<43:44, 10.50s/it] 59%|█████▊    | 353/602 [1:02:45<43:28, 10.48s/it] 59%|█████▉    | 354/602 [1:02:55<43:06, 10.43s/it] 59%|█████▉    | 355/602 [1:03:06<42:46, 10.39s/it] 59%|█████▉    | 356/602 [1:03:16<42:38, 10.40s/it] 59%|█████▉    | 357/602 [1:03:26<42:30, 10.41s/it] 59%|█████▉    | 358/602 [1:03:37<42:20, 10.41s/it] 60%|█████▉    | 359/602 [1:03:47<42:14, 10.43s/it] 60%|█████▉    | 360/602 [1:03:58<42:04, 10.43s/it]                                                    60%|█████▉    | 360/602 [1:03:58<42:04, 10.43s/it] 60%|█████▉    | 361/602 [1:04:09<42:23, 10.55s/it] 60%|██████    | 362/602 [1:04:19<41:54, 10.48s/it] 60%|██████    | 363/602 [1:04:29<41:46, 10.49s/it] 60%|██████    | 364/602 [1:04:40<41:25, 10.44s/it] 61%|██████    | 365/602 [1:04:50<41:19, 10.46s/it] 61%|██████    | 366/602 [1:05:00<40:47, 10.37s/it] 61%|██████    | 367/602 [1:05:11<40:44, 10.40s/it] 61%|██████    | 368/602 [1:05:21<40:33, 10.40s/it] 61%|██████▏   | 369/602 [1:05:32<40:28, 10.42s/it] 61%|██████▏   | 370/602 [1:05:42<40:31, 10.48s/it]                                                    61%|██████▏   | 370/602 [1:05:42<40:31, 10.48s/it] 62%|██████▏   | 371/602 [1:05:53<40:13, 10.45s/it] 62%|██████▏   | 372/602 [1:06:03<39:55, 10.41s/it] 62%|██████▏   | 373/602 [1:06:14<39:49, 10.43s/it] 62%|██████▏   | 374/602 [1:06:24<39:46, 10.47s/it] 62%|██████▏   | 375/602 [1:06:35<39:39, 10.48s/it] 62%|██████▏   | 376/602 [1:06:45<39:29, 10.49s/it] 63%|██████▎   | 377/602 [1:06:56<39:23, 10.51s/it] 63%|██████▎   | 378/602 [1:07:06<39:08, 10.48s/it] 63%|██████▎   | 379/602 [1:07:17<38:59, 10.49s/it] 63%|██████▎   | 380/602 [1:07:27<38:41, 10.46s/it]                                                    63%|██████▎   | 380/602 [1:07:27<38:41, 10.46s/it] 63%|██████▎   | 381/602 [1:07:37<38:08, 10.35s/it] 63%|██████▎   | 382/602 [1:07:48<38:07, 10.40s/it] 64%|██████▎   | 383/602 [1:07:58<37:54, 10.39s/it] 64%|██████▍   | 384/602 [1:08:09<38:00, 10.46s/it] 64%|██████▍   | 385/602 [1:08:19<37:49, 10.46s/it] 64%|██████▍   | 386/602 [1:08:30<37:51, 10.52s/it] 64%|██████▍   | 387/602 [1:08:40<37:29, 10.46s/it] 64%|██████▍   | 388/602 [1:08:51<37:29, 10.51s/it] 65%|██████▍   | 389/602 [1:09:01<37:12, 10.48s/it] 65%|██████▍   | 390/602 [1:09:12<37:08, 10.51s/it]                                                    65%|██████▍   | 390/602 [1:09:12<37:08, 10.51s/it] 65%|██████▍   | 391/602 [1:09:22<37:07, 10.56s/it] 65%|██████▌   | 392/602 [1:09:33<36:54, 10.55s/it] 65%|██████▌   | 393/602 [1:09:43<36:33, 10.49s/it] 65%|██████▌   | 394/602 [1:09:53<36:11, 10.44s/it] 66%|██████▌   | 395/602 [1:10:04<36:13, 10.50s/it] 66%|██████▌   | 396/602 [1:10:14<35:48, 10.43s/it] 66%|██████▌   | 397/602 [1:10:25<35:45, 10.47s/it] 66%|██████▌   | 398/602 [1:10:35<35:28, 10.43s/it] 66%|██████▋   | 399/602 [1:10:46<35:19, 10.44s/it] 66%|██████▋   | 400/602 [1:10:56<35:23, 10.51s/it]                                                    66%|██████▋   | 400/602 [1:10:56<35:23, 10.51s/it] 67%|██████▋   | 401/602 [1:11:07<35:19, 10.54s/it] 67%|██████▋   | 402/602 [1:11:18<35:27, 10.64s/it] 67%|██████▋   | 403/602 [1:11:28<35:04, 10.57s/it] 67%|██████▋   | 404/602 [1:11:39<34:42, 10.52s/it] 67%|██████▋   | 405/602 [1:11:49<34:25, 10.48s/it] 67%|██████▋   | 406/602 [1:12:00<34:29, 10.56s/it] 68%|██████▊   | 407/602 [1:12:10<34:10, 10.52s/it] 68%|██████▊   | 408/602 [1:12:21<34:16, 10.60s/it] 68%|██████▊   | 409/602 [1:12:32<34:07, 10.61s/it] 68%|██████▊   | 410/602 [1:12:42<33:35, 10.50s/it]                                                    68%|██████▊   | 410/602 [1:12:42<33:35, 10.50s/it] 68%|██████▊   | 411/602 [1:12:52<33:19, 10.47s/it] 68%|██████▊   | 412/602 [1:13:03<32:59, 10.42s/it] 69%|██████▊   | 413/602 [1:13:13<32:57, 10.46s/it] 69%|██████▉   | 414/602 [1:13:24<32:46, 10.46s/it] 69%|██████▉   | 415/602 [1:13:34<32:35, 10.45s/it] 69%|██████▉   | 416/602 [1:13:44<32:18, 10.42s/it] 69%|██████▉   | 417/602 [1:13:55<32:12, 10.44s/it] 69%|██████▉   | 418/602 [1:14:05<32:03, 10.45s/it] 70%|██████▉   | 419/602 [1:14:16<31:42, 10.40s/it] 70%|██████▉   | 420/602 [1:14:26<31:39, 10.44s/it]                                                    70%|██████▉   | 420/602 [1:14:26<31:39, 10.44s/it] 70%|██████▉   | 421/602 [1:14:37<31:21, 10.40s/it] 70%|███████   | 422/602 [1:14:47<31:23, 10.46s/it] 70%|███████   | 423/602 [1:14:58<31:12, 10.46s/it] 70%|███████   | 424/602 [1:15:08<31:04, 10.48s/it] 71%|███████   | 425/602 [1:15:19<30:51, 10.46s/it] 71%|███████   | 426/602 [1:15:29<30:37, 10.44s/it] 71%|███████   | 427/602 [1:15:39<30:22, 10.41s/it] 71%|███████   | 428/602 [1:15:50<30:26, 10.50s/it] 71%|███████▏  | 429/602 [1:16:00<30:16, 10.50s/it] 71%|███████▏  | 430/602 [1:16:11<29:49, 10.40s/it]                                                    71%|███████▏  | 430/602 [1:16:11<29:49, 10.40s/it] 72%|███████▏  | 431/602 [1:16:21<29:55, 10.50s/it] 72%|███████▏  | 432/602 [1:16:32<29:50, 10.54s/it] 72%|███████▏  | 433/602 [1:16:42<29:35, 10.50s/it] 72%|███████▏  | 434/602 [1:16:53<29:27, 10.52s/it] 72%|███████▏  | 435/602 [1:17:03<29:09, 10.47s/it] 72%|███████▏  | 436/602 [1:17:14<28:53, 10.44s/it] 73%|███████▎  | 437/602 [1:17:24<28:40, 10.43s/it] 73%|███████▎  | 438/602 [1:17:35<28:43, 10.51s/it] 73%|███████▎  | 439/602 [1:17:45<28:30, 10.50s/it] 73%|███████▎  | 440/602 [1:17:56<28:16, 10.47s/it]                                                    73%|███████▎  | 440/602 [1:17:56<28:16, 10.47s/it] 73%|███████▎  | 441/602 [1:18:06<28:06, 10.48s/it] 73%|███████▎  | 442/602 [1:18:17<28:00, 10.50s/it] 74%|███████▎  | 443/602 [1:18:27<27:56, 10.54s/it] 74%|███████▍  | 444/602 [1:18:38<27:40, 10.51s/it] 74%|███████▍  | 445/602 [1:18:48<27:34, 10.54s/it] 74%|███████▍  | 446/602 [1:18:59<27:17, 10.49s/it] 74%|███████▍  | 447/602 [1:19:09<27:03, 10.48s/it] 74%|███████▍  | 448/602 [1:19:20<26:50, 10.46s/it] 75%|███████▍  | 449/602 [1:19:30<26:41, 10.47s/it] 75%|███████▍  | 450/602 [1:19:41<26:43, 10.55s/it]                                                    75%|███████▍  | 450/602 [1:19:41<26:43, 10.55s/it] 75%|███████▍  | 451/602 [1:19:51<26:22, 10.48s/it] 75%|███████▌  | 452/602 [1:20:02<26:11, 10.47s/it] 75%|███████▌  | 453/602 [1:20:12<25:45, 10.37s/it] 75%|███████▌  | 454/602 [1:20:22<25:32, 10.36s/it] 76%|███████▌  | 455/602 [1:20:33<25:27, 10.39s/it] 76%|███████▌  | 456/602 [1:20:43<25:26, 10.46s/it] 76%|███████▌  | 457/602 [1:20:54<25:16, 10.46s/it] 76%|███████▌  | 458/602 [1:21:04<24:57, 10.40s/it] 76%|███████▌  | 459/602 [1:21:14<24:49, 10.42s/it] 76%|███████▋  | 460/602 [1:21:25<24:37, 10.40s/it]                                                    76%|███████▋  | 460/602 [1:21:25<24:37, 10.40s/it] 77%|███████▋  | 461/602 [1:21:35<24:26, 10.40s/it] 77%|███████▋  | 462/602 [1:21:45<24:12, 10.38s/it] 77%|███████▋  | 463/602 [1:21:56<23:57, 10.34s/it] 77%|███████▋  | 464/602 [1:22:06<23:54, 10.39s/it] 77%|███████▋  | 465/602 [1:22:17<23:49, 10.44s/it] 77%|███████▋  | 466/602 [1:22:27<23:42, 10.46s/it] 78%|███████▊  | 467/602 [1:22:38<23:35, 10.48s/it] 78%|███████▊  | 468/602 [1:22:48<23:29, 10.52s/it] 78%|███████▊  | 469/602 [1:22:59<23:20, 10.53s/it] 78%|███████▊  | 470/602 [1:23:09<23:07, 10.51s/it]                                                    78%|███████▊  | 470/602 [1:23:09<23:07, 10.51s/it] 78%|███████▊  | 471/602 [1:23:20<22:49, 10.46s/it] 78%|███████▊  | 472/602 [1:23:30<22:33, 10.41s/it] 79%|███████▊  | 473/602 [1:23:40<22:21, 10.40s/it] 79%|███████▊  | 474/602 [1:23:51<22:14, 10.43s/it] 79%|███████▉  | 475/602 [1:24:01<21:55, 10.36s/it] 79%|███████▉  | 476/602 [1:24:12<21:47, 10.38s/it] 79%|███████▉  | 477/602 [1:24:22<21:36, 10.37s/it] 79%|███████▉  | 478/602 [1:24:32<21:29, 10.40s/it] 80%|███████▉  | 479/602 [1:24:43<21:15, 10.37s/it] 80%|███████▉  | 480/602 [1:24:53<21:10, 10.41s/it]                                                    80%|███████▉  | 480/602 [1:24:53<21:10, 10.41s/it] 80%|███████▉  | 481/602 [1:25:04<20:58, 10.40s/it] 80%|████████  | 482/602 [1:25:14<20:59, 10.49s/it] 80%|████████  | 483/602 [1:25:25<20:52, 10.52s/it] 80%|████████  | 484/602 [1:25:35<20:37, 10.49s/it] 81%|████████  | 485/602 [1:25:46<20:27, 10.49s/it] 81%|████████  | 486/602 [1:25:56<20:12, 10.45s/it] 81%|████████  | 487/602 [1:26:06<19:55, 10.40s/it] 81%|████████  | 488/602 [1:26:17<19:49, 10.43s/it] 81%|████████  | 489/602 [1:26:27<19:42, 10.47s/it] 81%|████████▏ | 490/602 [1:26:38<19:27, 10.42s/it]                                                    81%|████████▏ | 490/602 [1:26:38<19:27, 10.42s/it] 82%|████████▏ | 491/602 [1:26:48<19:17, 10.43s/it] 82%|████████▏ | 492/602 [1:26:58<19:00, 10.37s/it] 82%|████████▏ | 493/602 [1:27:09<18:52, 10.39s/it] 82%|████████▏ | 494/602 [1:27:19<18:43, 10.40s/it] 82%|████████▏ | 495/602 [1:27:30<18:39, 10.46s/it] 82%|████████▏ | 496/602 [1:27:40<18:30, 10.47s/it] 83%|████████▎ | 497/602 [1:27:51<18:17, 10.46s/it] 83%|████████▎ | 498/602 [1:28:01<18:08, 10.47s/it] 83%|████████▎ | 499/602 [1:28:12<17:59, 10.48s/it] 83%|████████▎ | 500/602 [1:28:23<17:56, 10.56s/it]                                                    83%|████████▎ | 500/602 [1:28:23<17:56, 10.56s/it] 83%|████████▎ | 501/602 [1:28:33<17:45, 10.55s/it] 83%|████████▎ | 502/602 [1:28:43<17:29, 10.49s/it] 84%|████████▎ | 503/602 [1:28:54<17:16, 10.47s/it] 84%|████████▎ | 504/602 [1:29:04<17:01, 10.42s/it] 84%|████████▍ | 505/602 [1:29:15<16:53, 10.45s/it] 84%|████████▍ | 506/602 [1:29:25<16:48, 10.50s/it] 84%|████████▍ | 507/602 [1:29:36<16:42, 10.55s/it] 84%|████████▍ | 508/602 [1:29:47<16:37, 10.61s/it] 85%|████████▍ | 509/602 [1:29:57<16:25, 10.60s/it] 85%|████████▍ | 510/602 [1:30:08<16:17, 10.62s/it]                                                    85%|████████▍ | 510/602 [1:30:08<16:17, 10.62s/it] 85%|████████▍ | 511/602 [1:30:19<16:03, 10.59s/it] 85%|████████▌ | 512/602 [1:30:29<16:00, 10.67s/it] 85%|████████▌ | 513/602 [1:30:40<15:40, 10.56s/it] 85%|████████▌ | 514/602 [1:30:50<15:25, 10.51s/it] 86%|████████▌ | 515/602 [1:31:00<15:10, 10.46s/it] 86%|████████▌ | 516/602 [1:31:11<14:58, 10.45s/it] 86%|████████▌ | 517/602 [1:31:21<14:44, 10.40s/it] 86%|████████▌ | 518/602 [1:31:32<14:34, 10.41s/it] 86%|████████▌ | 519/602 [1:31:42<14:23, 10.41s/it] 86%|████████▋ | 520/602 [1:31:52<14:09, 10.36s/it]                                                    86%|████████▋ | 520/602 [1:31:52<14:09, 10.36s/it] 87%|████████▋ | 521/602 [1:32:03<13:59, 10.36s/it] 87%|████████▋ | 522/602 [1:32:13<13:52, 10.40s/it] 87%|████████▋ | 523/602 [1:32:24<13:42, 10.41s/it] 87%|████████▋ | 524/602 [1:32:34<13:39, 10.51s/it] 87%|████████▋ | 525/602 [1:32:45<13:25, 10.46s/it] 87%|████████▋ | 526/602 [1:32:55<13:20, 10.53s/it] 88%|████████▊ | 527/602 [1:33:06<13:07, 10.50s/it] 88%|████████▊ | 528/602 [1:33:16<12:57, 10.50s/it] 88%|████████▊ | 529/602 [1:33:27<12:47, 10.52s/it] 88%|████████▊ | 530/602 [1:33:37<12:37, 10.53s/it]                                                    88%|████████▊ | 530/602 [1:33:37<12:37, 10.53s/it] 88%|████████▊ | 531/602 [1:33:48<12:31, 10.58s/it] 88%|████████▊ | 532/602 [1:33:59<12:23, 10.62s/it] 89%|████████▊ | 533/602 [1:34:09<12:07, 10.54s/it] 89%|████████▊ | 534/602 [1:34:20<12:01, 10.61s/it] 89%|████████▉ | 535/602 [1:34:30<11:42, 10.48s/it] 89%|████████▉ | 536/602 [1:34:40<11:28, 10.43s/it] 89%|████████▉ | 537/602 [1:34:51<11:15, 10.40s/it] 89%|████████▉ | 538/602 [1:35:02<11:14, 10.54s/it] 90%|████████▉ | 539/602 [1:35:12<11:01, 10.49s/it] 90%|████████▉ | 540/602 [1:35:23<10:51, 10.51s/it]                                                    90%|████████▉ | 540/602 [1:35:23<10:51, 10.51s/it] 90%|████████▉ | 541/602 [1:35:33<10:43, 10.55s/it] 90%|█████████ | 542/602 [1:35:44<10:30, 10.52s/it] 90%|█████████ | 543/602 [1:35:54<10:17, 10.47s/it] 90%|█████████ | 544/602 [1:36:05<10:10, 10.53s/it] 91%|█████████ | 545/602 [1:36:15<09:59, 10.52s/it] 91%|█████████ | 546/602 [1:36:26<09:49, 10.53s/it] 91%|█████████ | 547/602 [1:36:36<09:35, 10.46s/it] 91%|█████████ | 548/602 [1:36:46<09:25, 10.48s/it] 91%|█████████ | 549/602 [1:36:57<09:13, 10.44s/it] 91%|█████████▏| 550/602 [1:37:07<09:03, 10.46s/it]                                                    91%|█████████▏| 550/602 [1:37:07<09:03, 10.46s/it] 92%|█████████▏| 551/602 [1:37:18<08:55, 10.51s/it] 92%|█████████▏| 552/602 [1:37:28<08:40, 10.41s/it] 92%|█████████▏| 553/602 [1:37:39<08:36, 10.53s/it] 92%|█████████▏| 554/602 [1:37:49<08:23, 10.48s/it] 92%|█████████▏| 555/602 [1:38:00<08:14, 10.53s/it] 92%|█████████▏| 556/602 [1:38:10<08:02, 10.49s/it] 93%|█████████▎| 557/602 [1:38:21<07:49, 10.43s/it] 93%|█████████▎| 558/602 [1:38:31<07:38, 10.43s/it] 93%|█████████▎| 559/602 [1:38:42<07:31, 10.50s/it] 93%|█████████▎| 560/602 [1:38:52<07:18, 10.44s/it]                                                    93%|█████████▎| 560/602 [1:38:52<07:18, 10.44s/it] 93%|█████████▎| 561/602 [1:39:02<07:06, 10.41s/it] 93%|█████████▎| 562/602 [1:39:13<06:59, 10.50s/it] 94%|█████████▎| 563/602 [1:39:23<06:45, 10.41s/it] 94%|█████████▎| 564/602 [1:39:34<06:37, 10.47s/it] 94%|█████████▍| 565/602 [1:39:44<06:27, 10.47s/it] 94%|█████████▍| 566/602 [1:39:55<06:17, 10.48s/it] 94%|█████████▍| 567/602 [1:40:05<06:07, 10.49s/it] 94%|█████████▍| 568/602 [1:40:16<05:55, 10.45s/it] 95%|█████████▍| 569/602 [1:40:26<05:46, 10.49s/it] 95%|█████████▍| 570/602 [1:40:37<05:34, 10.46s/it]                                                    95%|█████████▍| 570/602 [1:40:37<05:34, 10.46s/it] 95%|█████████▍| 571/602 [1:40:47<05:24, 10.47s/it] 95%|█████████▌| 572/602 [1:40:58<05:13, 10.46s/it] 95%|█████████▌| 573/602 [1:41:08<05:04, 10.49s/it] 95%|█████████▌| 574/602 [1:41:19<04:53, 10.49s/it] 96%|█████████▌| 575/602 [1:41:29<04:45, 10.56s/it] 96%|█████████▌| 576/602 [1:41:40<04:34, 10.55s/it] 96%|█████████▌| 577/602 [1:41:50<04:23, 10.53s/it] 96%|█████████▌| 578/602 [1:42:01<04:11, 10.46s/it] 96%|█████████▌| 579/602 [1:42:11<04:00, 10.45s/it] 96%|█████████▋| 580/602 [1:42:22<03:50, 10.49s/it]                                                    96%|█████████▋| 580/602 [1:42:22<03:50, 10.49s/it] 97%|█████████▋| 581/602 [1:42:32<03:39, 10.43s/it] 97%|█████████▋| 582/602 [1:42:42<03:28, 10.42s/it] 97%|█████████▋| 583/602 [1:42:53<03:17, 10.41s/it] 97%|█████████▋| 584/602 [1:43:03<03:08, 10.49s/it] 97%|█████████▋| 585/602 [1:43:14<02:58, 10.51s/it] 97%|█████████▋| 586/602 [1:43:24<02:47, 10.44s/it] 98%|█████████▊| 587/602 [1:43:35<02:38, 10.55s/it] 98%|█████████▊| 588/602 [1:43:45<02:26, 10.48s/it] 98%|█████████▊| 589/602 [1:43:56<02:16, 10.47s/it] 98%|█████████▊| 590/602 [1:44:06<02:04, 10.38s/it]                                                    98%|█████████▊| 590/602 [1:44:06<02:04, 10.38s/it] 98%|█████████▊| 591/602 [1:44:17<01:54, 10.44s/it] 98%|█████████▊| 592/602 [1:44:27<01:45, 10.57s/it] 99%|█████████▊| 593/602 [1:44:38<01:34, 10.50s/it] 99%|█████████▊| 594/602 [1:44:48<01:23, 10.44s/it] 99%|█████████▉| 595/602 [1:44:59<01:13, 10.44s/it] 99%|█████████▉| 596/602 [1:45:09<01:02, 10.46s/it] 99%|█████████▉| 597/602 [1:45:20<00:52, 10.45s/it] 99%|█████████▉| 598/602 [1:45:30<00:41, 10.49s/it]100%|█████████▉| 599/602 [1:45:41<00:31, 10.51s/it]100%|█████████▉| 600/602 [1:45:51<00:20, 10.47s/it]                                                   100%|█████████▉| 600/602 [1:45:51<00:20, 10.47s/it]100%|█████████▉| 601/602 [1:46:02<00:10, 10.49s/it]100%|██████████| 602/602 [1:46:12<00:00, 10.47s/it][INFO|trainer.py:3503] 2024-10-08 01:21:02,150 >> Saving model checkpoint to saves/llama3_lora_sft_random_all_cross_totally/checkpoint-602
[INFO|configuration_utils.py:731] 2024-10-08 01:21:02,173 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-08 01:21:02,174 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2024-10-08 01:21:02,315 >> tokenizer config file saved in saves/llama3_lora_sft_random_all_cross_totally/checkpoint-602/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-10-08 01:21:02,315 >> Special tokens file saved in saves/llama3_lora_sft_random_all_cross_totally/checkpoint-602/special_tokens_map.json
[INFO|trainer.py:2394] 2024-10-08 01:21:02,676 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 602/602 [1:46:13<00:00, 10.47s/it]100%|██████████| 602/602 [1:46:13<00:00, 10.59s/it]
[INFO|trainer.py:3503] 2024-10-08 01:21:02,682 >> Saving model checkpoint to saves/llama3_lora_sft_random_all_cross_totally
[INFO|configuration_utils.py:731] 2024-10-08 01:21:02,703 >> loading configuration file /home/sth/data/code/Meta-Llama-3-8B-Instruct/config.json
[INFO|configuration_utils.py:800] 2024-10-08 01:21:02,704 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.44.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2684] 2024-10-08 01:21:02,832 >> tokenizer config file saved in saves/llama3_lora_sft_random_all_cross_totally/tokenizer_config.json
[INFO|tokenization_utils_base.py:2693] 2024-10-08 01:21:02,833 >> Special tokens file saved in saves/llama3_lora_sft_random_all_cross_totally/special_tokens_map.json
{'loss': 0.176, 'grad_norm': 1.4670871496200562, 'learning_rate': 1.6393442622950818e-05, 'epoch': 0.02}
{'loss': 0.1234, 'grad_norm': 6.93564510345459, 'learning_rate': 3.2786885245901635e-05, 'epoch': 0.03}
{'loss': 0.1474, 'grad_norm': 1.7018383741378784, 'learning_rate': 4.918032786885246e-05, 'epoch': 0.05}
{'loss': 0.1009, 'grad_norm': 2.1014814376831055, 'learning_rate': 6.557377049180327e-05, 'epoch': 0.07}
{'loss': 0.0923, 'grad_norm': 1.2237781286239624, 'learning_rate': 8.19672131147541e-05, 'epoch': 0.08}
{'loss': 0.0943, 'grad_norm': 1.160908579826355, 'learning_rate': 9.836065573770493e-05, 'epoch': 0.1}
{'loss': 0.0941, 'grad_norm': 1.5671120882034302, 'learning_rate': 9.993172976674374e-05, 'epoch': 0.12}
{'loss': 0.11, 'grad_norm': 1.05631685256958, 'learning_rate': 9.969597324055187e-05, 'epoch': 0.13}
{'loss': 0.098, 'grad_norm': 1.3541055917739868, 'learning_rate': 9.929268216946794e-05, 'epoch': 0.15}
{'loss': 0.1032, 'grad_norm': 1.4733721017837524, 'learning_rate': 9.872321612413012e-05, 'epoch': 0.17}
{'loss': 0.1056, 'grad_norm': 0.7336342334747314, 'learning_rate': 9.798949488251068e-05, 'epoch': 0.18}
{'loss': 0.0893, 'grad_norm': 1.4762156009674072, 'learning_rate': 9.709399195798055e-05, 'epoch': 0.2}
{'loss': 0.1085, 'grad_norm': 0.8881406188011169, 'learning_rate': 9.603972626062696e-05, 'epoch': 0.22}
{'loss': 0.0911, 'grad_norm': 1.0820244550704956, 'learning_rate': 9.483025191993535e-05, 'epoch': 0.23}
{'loss': 0.0874, 'grad_norm': 0.9640012383460999, 'learning_rate': 9.346964630314521e-05, 'epoch': 0.25}
{'loss': 0.0975, 'grad_norm': 0.8318098187446594, 'learning_rate': 9.196249626967237e-05, 'epoch': 0.27}
{'loss': 0.0867, 'grad_norm': 1.3876221179962158, 'learning_rate': 9.03138827079365e-05, 'epoch': 0.28}
{'loss': 0.1022, 'grad_norm': 1.12919282913208, 'learning_rate': 8.852936340672324e-05, 'epoch': 0.3}
{'loss': 0.0739, 'grad_norm': 0.9586576819419861, 'learning_rate': 8.661495431882483e-05, 'epoch': 0.32}
{'loss': 0.0834, 'grad_norm': 1.0818699598312378, 'learning_rate': 8.457710928012301e-05, 'epoch': 0.33}
{'loss': 0.0768, 'grad_norm': 1.3022167682647705, 'learning_rate': 8.242269825248509e-05, 'epoch': 0.35}
{'loss': 0.0664, 'grad_norm': 0.8692994117736816, 'learning_rate': 8.015898416382026e-05, 'epoch': 0.37}
{'loss': 0.0729, 'grad_norm': 1.524775505065918, 'learning_rate': 7.779359842337321e-05, 'epoch': 0.38}
{'loss': 0.0882, 'grad_norm': 0.5692983269691467, 'learning_rate': 7.533451519479704e-05, 'epoch': 0.4}
{'loss': 0.0735, 'grad_norm': 0.8424354195594788, 'learning_rate': 7.27900245137362e-05, 'epoch': 0.42}
{'loss': 0.0646, 'grad_norm': 0.6318328380584717, 'learning_rate': 7.016870434054517e-05, 'epoch': 0.43}
{'loss': 0.0812, 'grad_norm': 0.4666849374771118, 'learning_rate': 6.747939164235819e-05, 'epoch': 0.45}
{'loss': 0.0696, 'grad_norm': 1.6151061058044434, 'learning_rate': 6.473115260199823e-05, 'epoch': 0.46}
{'loss': 0.0731, 'grad_norm': 1.0385682582855225, 'learning_rate': 6.193325205415629e-05, 'epoch': 0.48}
{'loss': 0.0794, 'grad_norm': 0.6691662669181824, 'learning_rate': 5.909512225187759e-05, 'epoch': 0.5}
{'loss': 0.066, 'grad_norm': 0.615944504737854, 'learning_rate': 5.622633106864895e-05, 'epoch': 0.51}
{'loss': 0.0547, 'grad_norm': 1.0773371458053589, 'learning_rate': 5.333654974328378e-05, 'epoch': 0.53}
{'loss': 0.0726, 'grad_norm': 0.7143182754516602, 'learning_rate': 5.043552027634293e-05, 'epoch': 0.55}
{'loss': 0.0507, 'grad_norm': 0.3583475649356842, 'learning_rate': 4.7533022588004445e-05, 'epoch': 0.56}
{'loss': 0.0595, 'grad_norm': 0.8467632532119751, 'learning_rate': 4.4638841548098956e-05, 'epoch': 0.58}
{'loss': 0.0622, 'grad_norm': 0.5558101534843445, 'learning_rate': 4.1762733989458965e-05, 'epoch': 0.6}
{'loss': 0.0604, 'grad_norm': 0.7249884605407715, 'learning_rate': 3.8914395815786045e-05, 'epoch': 0.61}
{'loss': 0.0541, 'grad_norm': 1.141466736793518, 'learning_rate': 3.610342931492182e-05, 'epoch': 0.63}
{'loss': 0.0537, 'grad_norm': 0.6037631630897522, 'learning_rate': 3.3339310787715665e-05, 'epoch': 0.65}
{'loss': 0.0436, 'grad_norm': 0.6630775332450867, 'learning_rate': 3.063135860161842e-05, 'epoch': 0.66}
{'loss': 0.0664, 'grad_norm': 0.7839376330375671, 'learning_rate': 2.7988701776699612e-05, 'epoch': 0.68}
{'loss': 0.0599, 'grad_norm': 0.7084744572639465, 'learning_rate': 2.542024920999047e-05, 'epoch': 0.7}
{'loss': 0.0513, 'grad_norm': 0.826214611530304, 'learning_rate': 2.293465964190362e-05, 'epoch': 0.71}
{'loss': 0.0457, 'grad_norm': 0.5053420066833496, 'learning_rate': 2.0540312465977863e-05, 'epoch': 0.73}
{'loss': 0.0558, 'grad_norm': 0.8651663064956665, 'learning_rate': 1.8245279480354504e-05, 'epoch': 0.75}
{'loss': 0.0629, 'grad_norm': 0.8605459928512573, 'learning_rate': 1.6057297676215832e-05, 'epoch': 0.76}
{'loss': 0.0534, 'grad_norm': 0.7900452017784119, 'learning_rate': 1.3983743154921503e-05, 'epoch': 0.78}
{'loss': 0.0502, 'grad_norm': 0.6457723379135132, 'learning_rate': 1.2031606261772805e-05, 'epoch': 0.8}
{'loss': 0.0532, 'grad_norm': 0.27385780215263367, 'learning_rate': 1.0207468020233663e-05, 'epoch': 0.81}
{'loss': 0.0479, 'grad_norm': 1.2012343406677246, 'learning_rate': 8.517477946053353e-06, 'epoch': 0.83}
{'loss': 0.059, 'grad_norm': 1.2527198791503906, 'learning_rate': 6.967333316083225e-06, 'epoch': 0.85}
{'loss': 0.0412, 'grad_norm': 0.6080239415168762, 'learning_rate': 5.562259961676691e-06, 'epoch': 0.86}
{'loss': 0.0538, 'grad_norm': 0.8421849012374878, 'learning_rate': 4.306994651421253e-06, 'epoch': 0.88}
{'loss': 0.0501, 'grad_norm': 0.6990121603012085, 'learning_rate': 3.2057691225937657e-06, 'epoch': 0.9}
{'loss': 0.0523, 'grad_norm': 1.0230951309204102, 'learning_rate': 2.2622958151720774e-06, 'epoch': 0.91}
{'loss': 0.0506, 'grad_norm': 0.6764737367630005, 'learning_rate': 1.4797553564961763e-06, 'epoch': 0.93}
{'loss': 0.0504, 'grad_norm': 0.7378606796264648, 'learning_rate': 8.607858387706336e-07, 'epoch': 0.95}
{'loss': 0.0509, 'grad_norm': 0.6790565252304077, 'learning_rate': 4.0747392555572914e-07, 'epoch': 0.96}
{'loss': 0.0416, 'grad_norm': 0.5926566123962402, 'learning_rate': 1.2134781722912824e-07, 'epoch': 0.98}
{'loss': 0.0502, 'grad_norm': 0.9882912635803223, 'learning_rate': 3.3720991327534924e-09, 'epoch': 1.0}
{'train_runtime': 6376.8041, 'train_samples_per_second': 12.089, 'train_steps_per_second': 0.094, 'train_loss': 0.07379801415426787, 'epoch': 1.0}
***** train metrics *****
  epoch                    =      0.9996
  total_flos               = 548245754GF
  train_loss               =      0.0738
  train_runtime            =  1:46:16.80
  train_samples_per_second =      12.089
  train_steps_per_second   =       0.094
Figure saved at: saves/llama3_lora_sft_random_all_cross_totally/training_loss.png
10/08/2024 01:21:03 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
10/08/2024 01:21:03 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.
[INFO|modelcard.py:449] 2024-10-08 01:21:03,076 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 🚀 View run [33msaves/llama3_lora_sft_random_all_cross_totally[0m at: [34mhttps://wandb.ai/beiweixiaoxu/huggingface/runs/5aebjmc3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241007_233447-5aebjmc3/logs[0m
